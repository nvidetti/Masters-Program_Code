{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nick Videtti\n",
      "IST 664 - Natural Language Processing - Winter 2023\n",
      "Homework 1\n",
      "February 5th, 2023\n",
      "\n",
      "This assignment will take two documents and perform a word tokenization on them.\n",
      "The two documents are Monty Python and the Holy Grail from the nltk.book package and a a text file containing prose that was\n",
      "copied and pasted from a Wikipedia page as on 2/4/2023 about History of the New York Knicks (https://en.wikipedia.org/wiki/History_of_the_New_York_Knicks).\n",
      "\n",
      "More will follow once the documents have been properly tokenized, but let's focus on that first!\n",
      "There are three fundamental steps to this initial tokenizaion:\n",
      "    1. Tokenizing the raw text. This will help split the documents into a list of its words.\n",
      "    2. Converting all tokens to be lowercase. This is done so that any given word is treated the same, regardless of capitalization.\n",
      "    3. Remove stopwords using the English stopwords provided by the NLTK. \n",
      "       This will make any anlysis on the tokens of each document much more meaningful, as stopwords tend to occur with high frequency and usually have much more to do with grammar than with context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Nick Videtti\n",
    "IST 664 - Natural Language Processing - Winter 2023\n",
    "Homework 1\n",
    "February 5th, 2023\n",
    "\n",
    "This assignment will take two documents and perform a word tokenization on them.\n",
    "The two documents are Monty Python and the Holy Grail from the nltk.book package and a a text file containing prose that was\n",
    "copied and pasted from a Wikipedia page as on 2/4/2023 about History of the New York Knicks (https://en.wikipedia.org/wiki/History_of_the_New_York_Knicks).\n",
    "\n",
    "More will follow once the documents have been properly tokenized, but let's focus on that first!\n",
    "There are three fundamental steps to this initial tokenizaion:\n",
    "    1. Tokenizing the raw text. This will help split the documents into a list of its words.\n",
    "    2. Converting all tokens to be lowercase. This is done so that any given word is treated the same, regardless of capitalization.\n",
    "    3. Remove stopwords using the English stopwords provided by the NLTK. \n",
    "       This will make any anlysis on the tokens of each document much more meaningful, as stopwords tend to occur with high frequency and usually have much more to do with grammar than with context.\n",
    "''')\n",
    "\n",
    "#Import the Natural Language Toolkit and the nltk.book package\n",
    "import nltk\n",
    "from nltk import book\n",
    "\n",
    "#Monty Python and the Holy Grail tokens\n",
    "Monty_Python_tokens_RAW = nltk.book.text6.tokens\n",
    "\n",
    "#Knicks History with tokenizing\n",
    "#source: Wikipedia - https://en.wikipedia.org/wiki/History_of_the_New_York_Knicks\n",
    "with open('C:/Users/nvidetti/Downloads/History of Knicks Wiki.txt','r', encoding = 'utf-8') as Knicks_History_Text_File: Knicks_History = Knicks_History_Text_File.read()\n",
    "Knicks_History_tokens_RAW = nltk.word_tokenize(Knicks_History)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY OF INITIAL TOKENIZATION\n",
      "\n",
      "Monty Python and The Holy Grail\n",
      "Raw Text: 16967 tokens\n",
      "Lowercase Text: 16967 tokens, 4113 changed\n",
      "Lowercase without Stopwords: 12209 tokens 4758 removed\n",
      "SUMMARY OF INITIAL TOKENIZATION\n",
      "\n",
      "Knicks History\n",
      "Raw Text: 10925 tokens\n",
      "Lowercase Text: 10925 tokens, 2096 changed\n",
      "Lowercase without Stopwords: 7457 tokens, 3468 removed\n",
      "\n",
      "\n",
      "Let's take a quick look at the first few tokens in each document.\n",
      "First 10 Monty Python Tokens: ['scene', '1', ':', '[', 'wind', ']', '[', 'clop', 'clop', 'clop']\n",
      "First 10 Knicks History Tokens: ['new', 'york', 'knicks', 'one', 'oldest', 'teams', 'national', 'basketball', 'association', '(']\n",
      "\n",
      "Looks like there is still more cleaning to do. Both lists of tokens have non-alphabetic tokens and this will need to be addressed in otder to focus on only words in the text.\n"
     ]
    }
   ],
   "source": [
    "#CHANGE ALL TOKENS TO LOWERCASE\n",
    "Monty_Python_tokens_lower = [token.lower() for token in Monty_Python_tokens_RAW]\n",
    "Knicks_History_tokens_lower = [token.lower() for token in Knicks_History_tokens_RAW]\n",
    "\n",
    "#REMOVE STOPWORDS\n",
    "Monty_Python_tokens = [token for token in Monty_Python_tokens_lower if not token in nltk.corpus.stopwords.words('english')]\n",
    "Knicks_History_tokens = [token for token in Knicks_History_tokens_lower if not token in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "#Number of tokens in each step\n",
    "#Monty Python\n",
    "print('SUMMARY OF INITIAL TOKENIZATION\\n\\nMonty Python and The Holy Grail\\nRaw Text:', len(Monty_Python_tokens_RAW), 'tokens')\n",
    "print('Lowercase Text:', len(Monty_Python_tokens_lower), 'tokens,', len([element for element in range(len(Monty_Python_tokens_lower)) if Monty_Python_tokens_lower[element] != Monty_Python_tokens_RAW[element]]), 'changed')\n",
    "print('Lowercase without Stopwords:', len(Monty_Python_tokens), 'tokens', len(Monty_Python_tokens_lower) - len(Monty_Python_tokens), 'removed')\n",
    "#Knicks_History\n",
    "print('SUMMARY OF INITIAL TOKENIZATION\\n\\nKnicks History\\nRaw Text:', len(Knicks_History_tokens_RAW), 'tokens')\n",
    "print('Lowercase Text:', len(Knicks_History_tokens_lower), 'tokens,', len([element for element in range(len(Knicks_History_tokens_lower)) if Knicks_History_tokens_lower[element] != Knicks_History_tokens_RAW[element]]), 'changed')\n",
    "print('Lowercase without Stopwords:', len(Knicks_History_tokens), 'tokens,', len(Knicks_History_tokens_lower) - len(Knicks_History_tokens), 'removed')\n",
    "\n",
    "\n",
    "print('\\n\\nLet\\'s take a quick look at the first few tokens in each document.')\n",
    "print('First 10 Monty Python Tokens:', Monty_Python_tokens[:10])\n",
    "print('First 10 Knicks History Tokens:', Knicks_History_tokens[:10])\n",
    "\n",
    "print('\\nLooks like there is still more cleaning to do. Both lists of tokens have non-alphabetic tokens and this will need to be addressed in otder to focus on only words in the text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATED NUMBER OF TOKENS\n",
      "Monty Python: 6692 tokens, 5517 removed\n",
      "Knicks History: 4951 tokens, 2506 removed\n",
      "\n",
      "\n",
      "Let's take another look at the first few tokens in each document.\n",
      "First 10 Monty Python Tokens: ['scene', 'wind', 'clop', 'clop', 'clop', 'king', 'arthur', 'whoa', 'clop', 'clop']\n",
      "First 10 Knicks History Tokens: ['new', 'york', 'knicks', 'one', 'oldest', 'teams', 'national', 'basketball', 'association', 'nba']\n"
     ]
    }
   ],
   "source": [
    "#Remove non-alphabetic tokens\n",
    "import re\n",
    "print('UPDATED NUMBER OF TOKENS')\n",
    "print('Monty Python:', len([token for token in Monty_Python_tokens if re.compile('^[a-z]+$').match(token)]), 'tokens,', \\\n",
    "    len(Monty_Python_tokens) - len([token for token in Monty_Python_tokens if re.compile('^[a-z]+$').match(token)]), 'removed')\n",
    "print('Knicks History:', len([token for token in Knicks_History_tokens if re.compile('^[a-z]+$').match(token)]), 'tokens,', \\\n",
    "    len(Knicks_History_tokens) - len([token for token in Knicks_History_tokens if re.compile('^[a-z]+$').match(token)]), 'removed')\n",
    "\n",
    "Monty_Python_tokens = [token for token in Monty_Python_tokens if re.compile('^[a-z]+$').match(token)]\n",
    "Knicks_History_tokens = [token for token in Knicks_History_tokens if re.compile('^[a-z]+$').match(token)]\n",
    "\n",
    "print('\\n\\nLet\\'s take another look at the first few tokens in each document.')\n",
    "print('First 10 Monty Python Tokens:', Monty_Python_tokens[:10])\n",
    "print('First 10 Knicks History Tokens:', Knicks_History_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Looks good! Now that we have our cleaned up tokens, we can do some analysis.\n",
      "The following will be done for both lists of tokens:\n",
      "    1. Top 50 Tokens by Frequency\n",
      "    2. Top 50 Tokens by Normalized Frequency\n",
      "    3. Top 50 Bigrams by Normalized Frequency\n",
      "    4. Top 50 Bigrams by PMI (Pointwise Mutual Information). \n",
      "       This anlysis will only be done on bigrams that occur at least 5 times in their respective document to make the PMI calculation more meaningful.\n",
      "\n",
      "\n",
      "TOP 50 MONTY PYTHON TOKENS BY FREQUENCY\n",
      "arthur 261\n",
      "oh 112\n",
      "launcelot 101\n",
      "knight 84\n",
      "galahad 80\n",
      "father 75\n",
      "sir 72\n",
      "ni 69\n",
      "bedevere 67\n",
      "knights 65\n",
      "well 62\n",
      "head 59\n",
      "ha 59\n",
      "robin 58\n",
      "right 55\n",
      "guard 55\n",
      "yes 53\n",
      "villager 47\n",
      "boom 45\n",
      "come 44\n",
      "uh 42\n",
      "witch 41\n",
      "clop 39\n",
      "away 39\n",
      "grail 39\n",
      "king 38\n",
      "one 37\n",
      "black 36\n",
      "burn 36\n",
      "french 35\n",
      "tim 34\n",
      "us 32\n",
      "look 31\n",
      "singing 31\n",
      "scene 30\n",
      "get 30\n",
      "dead 30\n",
      "mumble 30\n",
      "go 29\n",
      "music 29\n",
      "squeak 29\n",
      "herbert 28\n",
      "got 27\n",
      "tell 27\n",
      "hello 27\n",
      "camelot 26\n",
      "holy 26\n",
      "castle 25\n",
      "dennis 25\n",
      "soldier 24\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "\n",
    "\n",
    "Looks good! Now that we have our cleaned up tokens, we can do some analysis.\n",
    "The following will be done for both lists of tokens:\n",
    "    1. Top 50 Tokens by Frequency\n",
    "    2. Top 50 Tokens by Normalized Frequency\n",
    "    3. Top 50 Bigrams by Normalized Frequency\n",
    "    4. Top 50 Bigrams by PMI (Pointwise Mutual Information). \n",
    "       This anlysis will only be done on bigrams that occur at least 5 times in their respective document to make the PMI calculation more meaningful.\n",
    "\n",
    "''')\n",
    "\n",
    "print('TOP 50 MONTY PYTHON TOKENS BY FREQUENCY')\n",
    "for row in nltk.FreqDist(Monty_Python_tokens).most_common()[:50]: print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 50 KNICKS HISTORY TOKENS BY FREQUENCY\n",
      "knicks 213\n",
      "season 95\n",
      "team 87\n",
      "first 70\n",
      "record 65\n",
      "nba 57\n",
      "game 47\n",
      "coach 44\n",
      "new 43\n",
      "games 43\n",
      "round 33\n",
      "york 31\n",
      "head 30\n",
      "playoffs 27\n",
      "however 26\n",
      "guard 25\n",
      "finals 25\n",
      "franchise 24\n",
      "year 24\n",
      "draft 23\n",
      "playoff 23\n",
      "players 22\n",
      "second 22\n",
      "one 21\n",
      "two 20\n",
      "eastern 20\n",
      "ewing 20\n",
      "garden 19\n",
      "would 19\n",
      "series 19\n",
      "division 18\n",
      "losing 18\n",
      "win 18\n",
      "also 18\n",
      "point 18\n",
      "thomas 18\n",
      "time 17\n",
      "history 17\n",
      "winning 17\n",
      "lost 17\n",
      "madison 16\n",
      "square 16\n",
      "named 16\n",
      "former 16\n",
      "basketball 15\n",
      "years 15\n",
      "points 15\n",
      "following 15\n",
      "bulls 15\n",
      "pacers 15\n"
     ]
    }
   ],
   "source": [
    "print('\\nTOP 50 KNICKS HISTORY TOKENS BY FREQUENCY')\n",
    "for row in nltk.FreqDist(Knicks_History_tokens).most_common()[:50]: print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 50 MONTY PYTHON TOKENS BY NORMALIZED FREQUENCY\n",
      "arthur 0.039001793185893606\n",
      "oh 0.016736401673640166\n",
      "launcelot 0.015092647937836223\n",
      "knight 0.012552301255230125\n",
      "galahad 0.011954572624028692\n",
      "father 0.011207411835026897\n",
      "sir 0.010759115361625823\n",
      "ni 0.010310818888224747\n",
      "bedevere 0.010011954572624028\n",
      "knights 0.009713090257023311\n",
      "well 0.009264793783622235\n",
      "head 0.008816497310221159\n",
      "ha 0.008816497310221159\n",
      "robin 0.008667065152420802\n",
      "right 0.008218768679019725\n",
      "guard 0.008218768679019725\n",
      "yes 0.007919904363419009\n",
      "villager 0.0070233114166168556\n",
      "boom 0.006724447101016139\n",
      "come 0.00657501494321578\n",
      "uh 0.006276150627615063\n",
      "witch 0.006126718469814704\n",
      "clop 0.005827854154213987\n",
      "away 0.005827854154213987\n",
      "grail 0.005827854154213987\n",
      "king 0.005678421996413628\n",
      "one 0.00552898983861327\n",
      "black 0.005379557680812911\n",
      "burn 0.005379557680812911\n",
      "french 0.005230125523012552\n",
      "tim 0.005080693365212194\n",
      "us 0.004781829049611476\n",
      "look 0.004632396891811118\n",
      "singing 0.004632396891811118\n",
      "scene 0.004482964734010759\n",
      "get 0.004482964734010759\n",
      "dead 0.004482964734010759\n",
      "mumble 0.004482964734010759\n",
      "go 0.004333532576210401\n",
      "music 0.004333532576210401\n",
      "squeak 0.004333532576210401\n",
      "herbert 0.0041841004184100415\n",
      "got 0.004034668260609683\n",
      "tell 0.004034668260609683\n",
      "hello 0.004034668260609683\n",
      "camelot 0.0038852361028093247\n",
      "holy 0.0038852361028093247\n",
      "castle 0.003735803945008966\n",
      "dennis 0.003735803945008966\n",
      "soldier 0.0035863717872086074\n"
     ]
    }
   ],
   "source": [
    "print('\\nTOP 50 MONTY PYTHON TOKENS BY NORMALIZED FREQUENCY')\n",
    "for row in nltk.FreqDist(Monty_Python_tokens).most_common()[:50]: print(row[0], row[1] / len(Monty_Python_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 50 KNICKS HISTORY TOKENS BY NORMALIZED FREQUENCY\n",
      "knicks 0.04302161179559685\n",
      "season 0.0191880428196324\n",
      "team 0.017572207634821247\n",
      "first 0.014138557867097556\n",
      "record 0.013128660876590587\n",
      "nba 0.011512825691779438\n",
      "game 0.009493031710765503\n",
      "coach 0.00888709351646132\n",
      "new 0.008685114118359927\n",
      "games 0.008685114118359927\n",
      "round 0.006665320137345991\n",
      "york 0.006261361341143203\n",
      "head 0.0060593819430418095\n",
      "playoffs 0.005453443748737629\n",
      "however 0.005251464350636235\n",
      "guard 0.005049484952534842\n",
      "finals 0.005049484952534842\n",
      "franchise 0.004847505554433448\n",
      "year 0.004847505554433448\n",
      "draft 0.004645526156332054\n",
      "playoff 0.004645526156332054\n",
      "players 0.00444354675823066\n",
      "second 0.00444354675823066\n",
      "one 0.004241567360129267\n",
      "two 0.004039587962027873\n",
      "eastern 0.004039587962027873\n",
      "ewing 0.004039587962027873\n",
      "garden 0.0038376085639264795\n",
      "would 0.0038376085639264795\n",
      "series 0.0038376085639264795\n",
      "division 0.003635629165825086\n",
      "losing 0.003635629165825086\n",
      "win 0.003635629165825086\n",
      "also 0.003635629165825086\n",
      "point 0.003635629165825086\n",
      "thomas 0.003635629165825086\n",
      "time 0.0034336497677236923\n",
      "history 0.0034336497677236923\n",
      "winning 0.0034336497677236923\n",
      "lost 0.0034336497677236923\n",
      "madison 0.0032316703696222983\n",
      "square 0.0032316703696222983\n",
      "named 0.0032316703696222983\n",
      "former 0.0032316703696222983\n",
      "basketball 0.0030296909715209048\n",
      "years 0.0030296909715209048\n",
      "points 0.0030296909715209048\n",
      "following 0.0030296909715209048\n",
      "bulls 0.0030296909715209048\n",
      "pacers 0.0030296909715209048\n"
     ]
    }
   ],
   "source": [
    "print('\\nTOP 50 KNICKS HISTORY TOKENS BY NORMALIZED FREQUENCY')\n",
    "for row in nltk.FreqDist(Knicks_History_tokens).most_common()[:50]: print(row[0], row[1] / len(Knicks_History_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 50 MONTY PYTHON BIGRAMS BY NORMALIZED FREQUENCY\n",
      "('ha', 'ha') 0.00552898983861327\n",
      "('black', 'knight') 0.004931261207411835\n",
      "('head', 'knight') 0.004333532576210401\n",
      "('clop', 'clop') 0.0038852361028093247\n",
      "('hello', 'hello') 0.00328750747160789\n",
      "('french', 'guard') 0.0031380753138075313\n",
      "('mumble', 'mumble') 0.002988643156007173\n",
      "('witch', 'witch') 0.002988643156007173\n",
      "('burn', 'burn') 0.002839210998206814\n",
      "('holy', 'grail') 0.002839210998206814\n",
      "('king', 'arthur') 0.002839210998206814\n",
      "('sir', 'robin') 0.002839210998206814\n",
      "('squeak', 'squeak') 0.002839210998206814\n",
      "('knights', 'ni') 0.0026897788404064557\n",
      "('ni', 'ni') 0.0026897788404064557\n",
      "('run', 'away') 0.0026897788404064557\n",
      "('sir', 'launcelot') 0.0026897788404064557\n",
      "('saw', 'saw') 0.0022414823670053796\n",
      "('brave', 'sir') 0.0019426180514046623\n",
      "('cart', 'master') 0.0019426180514046623\n",
      "('going', 'tell') 0.0019426180514046623\n",
      "('arthur', 'well') 0.0017931858936043037\n",
      "('cartoon', 'character') 0.0017931858936043037\n",
      "('minstrel', 'singing') 0.0017931858936043037\n",
      "('arthur', 'knights') 0.001643753735803945\n",
      "('bring', 'dead') 0.001643753735803945\n",
      "('guests', 'singing') 0.001643753735803945\n",
      "('old', 'man') 0.001643753735803945\n",
      "('arthur', 'king') 0.0014943215780035865\n",
      "('clang', 'bring') 0.0014943215780035865\n",
      "('clap', 'clap') 0.0014943215780035865\n",
      "('heh', 'heh') 0.0014943215780035865\n",
      "('iesu', 'domine') 0.0014943215780035865\n",
      "('left', 'head') 0.0014943215780035865\n",
      "('ni', 'arthur') 0.0014943215780035865\n",
      "('pie', 'iesu') 0.0014943215780035865\n",
      "('arthur', 'look') 0.0013448894202032278\n",
      "('arthur', 'music') 0.0013448894202032278\n",
      "('arthur', 'yes') 0.0013448894202032278\n",
      "('away', 'run') 0.0013448894202032278\n",
      "('boom', 'boom') 0.0013448894202032278\n",
      "('guard', 'hic') 0.0013448894202032278\n",
      "('head', 'oh') 0.0013448894202032278\n",
      "('launcelot', 'well') 0.0013448894202032278\n",
      "('mumble', 'boom') 0.0013448894202032278\n",
      "('music', 'stops') 0.0013448894202032278\n",
      "('round', 'table') 0.0013448894202032278\n",
      "('sir', 'galahad') 0.0013448894202032278\n",
      "('bedevere', 'ni') 0.001195457262402869\n",
      "('dead', 'clang') 0.001195457262402869\n"
     ]
    }
   ],
   "source": [
    "print('\\nTOP 50 MONTY PYTHON BIGRAMS BY NORMALIZED FREQUENCY')\n",
    "for row in nltk.BigramCollocationFinder.from_words(Monty_Python_tokens).score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq)[:50]: print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 50 KNICKS HISTORY BIGRAMS BY NORMALIZED FREQUENCY\n",
      "('new', 'york') 0.006261361341143203\n",
      "('head', 'coach') 0.005857402544940416\n",
      "('first', 'round') 0.00444354675823066\n",
      "('madison', 'square') 0.0032316703696222983\n",
      "('season', 'knicks') 0.0032316703696222983\n",
      "('square', 'garden') 0.0032316703696222983\n",
      "('point', 'guard') 0.0026257321753181176\n",
      "('eastern', 'conference') 0.00222177337911533\n",
      "('second', 'round') 0.0020197939810139365\n",
      "('eastern', 'division') 0.001817814582912543\n",
      "('first', 'time') 0.001817814582912543\n",
      "('nba', 'draft') 0.001817814582912543\n",
      "('round', 'playoffs') 0.001817814582912543\n",
      "('season', 'record') 0.001817814582912543\n",
      "('chicago', 'bulls') 0.0016158351848111492\n",
      "('five', 'games') 0.0016158351848111492\n",
      "('season', 'team') 0.0016158351848111492\n",
      "('franchise', 'history') 0.0014138557867097556\n",
      "('houston', 'rockets') 0.0014138557867097556\n",
      "('however', 'knicks') 0.0014138557867097556\n",
      "('knicks', 'also') 0.0014138557867097556\n",
      "('knicks', 'lost') 0.0014138557867097556\n",
      "('los', 'angeles') 0.0014138557867097556\n",
      "('amar', 'e') 0.001211876388608362\n",
      "('carmelo', 'anthony') 0.001211876388608362\n",
      "('conference', 'finals') 0.001211876388608362\n",
      "('dallas', 'mavericks') 0.001211876388608362\n",
      "('donnie', 'walsh') 0.001211876388608362\n",
      "('e', 'stoudemire') 0.001211876388608362\n",
      "('following', 'season') 0.001211876388608362\n",
      "('indiana', 'pacers') 0.001211876388608362\n",
      "('isiah', 'thomas') 0.001211876388608362\n",
      "('knicks', 'struggled') 0.001211876388608362\n",
      "('nba', 'finals') 0.001211876388608362\n",
      "('patrick', 'ewing') 0.001211876388608362\n",
      "('willis', 'reed') 0.001211876388608362\n",
      "('angeles', 'lakers') 0.0010098969905069683\n",
      "('atlantic', 'division') 0.0010098969905069683\n",
      "('boston', 'celtics') 0.0010098969905069683\n",
      "('coach', 'knicks') 0.0010098969905069683\n",
      "('first', 'season') 0.0010098969905069683\n",
      "('games', 'season') 0.0010098969905069683\n",
      "('knicks', 'faced') 0.0010098969905069683\n",
      "('knicks', 'finished') 0.0010098969905069683\n",
      "('nba', 'rookie') 0.0010098969905069683\n",
      "('per', 'game') 0.0010098969905069683\n",
      "('phil', 'jackson') 0.0010098969905069683\n",
      "('record', 'season') 0.0010098969905069683\n",
      "('regular', 'season') 0.0010098969905069683\n",
      "('team', 'president') 0.0010098969905069683\n"
     ]
    }
   ],
   "source": [
    "print('\\nTOP 50 KNICKS HISTORY BIGRAMS BY NORMALIZED FREQUENCY')\n",
    "for row in nltk.BigramCollocationFinder.from_words(Knicks_History_tokens).score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq)[:50]: print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 50 MONTY PYTHON BIGRAMS BY PMI\n",
      "('monks', 'chanting') 10.123259229317197\n",
      "('dramatic', 'chord') 9.900866807980748\n",
      "('dona', 'eis') 9.708221730038353\n",
      "('eis', 'requiem') 9.708221730038353\n",
      "('hand', 'grenade') 9.708221730038353\n",
      "('angels', 'sing') 9.637832402146955\n",
      "('iesu', 'domine') 9.386293635150992\n",
      "('pie', 'iesu') 9.386293635150992\n",
      "('round', 'table') 9.248790111401057\n",
      "('haw', 'haw') 9.175726649211333\n",
      "('chanting', 'pie') 9.123259229317195\n",
      "('bridge', 'death') 8.971256135872146\n",
      "('domine', 'dona') 8.708221730038353\n",
      "('hold', 'hold') 8.69029982204109\n",
      "('cartoon', 'character') 8.620758888788012\n",
      "('saw', 'saw') 8.440186643146195\n",
      "('cart', 'master') 8.431381524679527\n",
      "('rewr', 'rewr') 8.386293635150992\n",
      "('make', 'sure') 8.21636863370868\n",
      "('clap', 'clap') 8.216368633708678\n",
      "('brother', 'maynard') 8.123259229317195\n",
      "('clang', 'bring') 8.035796388066856\n",
      "('court', 'camelot') 8.00778201189726\n",
      "('shh', 'shh') 7.894440538821316\n",
      "('thank', 'thank') 7.678474386644301\n",
      "('hello', 'hello') 7.657878344348713\n",
      "('minstrel', 'singing') 7.63854820223154\n",
      "('hee', 'hee') 7.533296047537675\n",
      "('bring', 'dead') 7.353872157458612\n",
      "('old', 'man') 7.3347633345109085\n",
      "('french', 'guards') 7.257010618206024\n",
      "('squeak', 'squeak') 7.240187253226795\n",
      "('dead', 'person') 7.216368633708679\n",
      "('mumble', 'mumble') 7.216368633708678\n",
      "('music', 'stops') 7.212810814295489\n",
      "('going', 'tell') 7.194342327378678\n",
      "('three', 'questions') 7.163901213814542\n",
      "('run', 'away') 7.13331289398112\n",
      "('heh', 'heh') 7.11128658765112\n",
      "('guests', 'singing') 7.043532036846463\n",
      "('king', 'britons') 7.00086259795747\n",
      "('holy', 'grail') 6.970307306478599\n",
      "('guard', 'hic') 6.926862016513693\n",
      "('clop', 'clop') 6.837857010454947\n",
      "('dead', 'clang') 6.713868293179495\n",
      "('please', 'please') 6.690299822041091\n",
      "('middle', 'head') 6.6556536792342005\n",
      "('quest', 'holy') 6.629270388643532\n",
      "('burn', 'burn') 6.616299240597314\n",
      "('left', 'head') 6.562544274842718\n"
     ]
    }
   ],
   "source": [
    "print('\\nTOP 50 MONTY PYTHON BIGRAMS BY PMI')\n",
    "pmi_monty_finder = nltk.BigramCollocationFinder.from_words(Monty_Python_tokens)\n",
    "pmi_monty_finder.apply_freq_filter(5)\n",
    "for row in pmi_monty_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().pmi)[:50]: print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 50 KNICKS HISTORY BIGRAMS BY NORMALIZED PMI\n",
      "('amar', 'e') 9.688541733241005\n",
      "('los', 'angeles') 9.466149311904555\n",
      "('donnie', 'walsh') 9.27350423396216\n",
      "('e', 'stoudemire') 8.951576139074799\n",
      "('willis', 'reed') 8.814072615324864\n",
      "('angeles', 'lakers') 8.618152405349605\n",
      "('dallas', 'mavericks') 8.536538639795953\n",
      "('phil', 'jackson') 8.466149311904555\n",
      "('carmelo', 'anthony') 8.366613638353641\n",
      "('indiana', 'pacers') 8.366613638353641\n",
      "('madison', 'square') 8.27350423396216\n",
      "('boston', 'celtics') 8.203114906070763\n",
      "('chicago', 'bulls') 8.196688636911329\n",
      "('atlantic', 'division') 8.103579232519849\n",
      "('houston', 'rockets') 8.103579232519849\n",
      "('square', 'garden') 8.025576720518576\n",
      "('patrick', 'ewing') 7.951576139074798\n",
      "('eastern', 'conference') 7.8260452569909384\n",
      "('isiah', 'thomas') 7.366613638353642\n",
      "('point', 'guard') 7.160162760886216\n",
      "('eastern', 'division') 6.951576139074799\n",
      "('new', 'york') 6.847239479260065\n",
      "('head', 'coach') 6.7651630148439175\n",
      "('per', 'game') 6.718915382284523\n",
      "('conference', 'finals') 6.6296480441874355\n",
      "('five', 'games') 6.525311384372701\n",
      "('franchise', 'history') 6.408433814048269\n",
      "('second', 'round') 6.091606590853772\n",
      "('nba', 'rookie') 5.9551873926271774\n",
      "('regular', 'season') 5.703648625631212\n",
      "('round', 'playoffs') 5.64414761388255\n",
      "('first', 'round') 5.559258716296037\n",
      "('first', 'time') 5.2266833772091665\n",
      "('nba', 'draft') 5.086977265182718\n",
      "('two', 'games') 4.847239479260063\n",
      "('team', 'president') 4.452049114859703\n",
      "('following', 'season') 4.38172053074385\n",
      "('nba', 'finals') 4.38172053074385\n",
      "('knicks', 'faced') 3.690797707181371\n",
      "('knicks', 'finished') 3.690797707181371\n",
      "('knicks', 'struggled') 3.4233173963163868\n",
      "('knicks', 'lost') 3.258686694543586\n",
      "('knicks', 'also') 3.176224534351613\n",
      "('season', 'record') 2.85120581404507\n",
      "('however', 'knicks') 2.6457098176528344\n",
      "('games', 'season') 2.599311965816476\n",
      "('season', 'team') 2.260705129782485\n",
      "('record', 'season') 2.0032089074901194\n",
      "('season', 'knicks') 1.9689390054053746\n",
      "('first', 'season') 1.8962937035736083\n"
     ]
    }
   ],
   "source": [
    "print('\\nTOP 50 KNICKS HISTORY BIGRAMS BY NORMALIZED PMI')\n",
    "pmi_knicks_finder = nltk.BigramCollocationFinder.from_words(Knicks_History_tokens)\n",
    "pmi_knicks_finder.apply_freq_filter(5)\n",
    "for row in pmi_knicks_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().pmi)[:50]: print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "As expected, these are two very different results. \n",
      "The Monty Python word tokens and bigrams can give a bit of context to the fictional story and the Knicks History word tokens and bigrams can give context as to people and teams that have had a big impact in Knicks history.\n",
      "\n",
      "One interesting observation is the repeating of words in the Monty Python bigrams. It is intuitive to assume that this happens rarely or never in the Knicks history bigrams, but there is only one way to find out!\n",
      "We will look at all the bigrams that occur where both elements are the same, hence find the bigrams in each document that consist of the same word being used consecutively.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "\n",
    "As expected, these are two very different results. \n",
    "The Monty Python word tokens and bigrams can give a bit of context to the fictional story and the Knicks History word tokens and bigrams can give context as to people and teams that have had a big impact in Knicks history.\n",
    "\n",
    "One interesting observation is the repeating of words in the Monty Python bigrams. It is intuitive to assume that this happens rarely or never in the Knicks history bigrams, but there is only one way to find out!\n",
    "We will look at all the bigrams that occur where both elements are the same, hence find the bigrams in each document that consist of the same word being used consecutively.\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MONTY PYTHON BIGRAMS WITH SAME TWO WORDS\n",
      "('ha', 'ha') 0.00552898983861327\n",
      "('clop', 'clop') 0.0038852361028093247\n",
      "('hello', 'hello') 0.00328750747160789\n",
      "('mumble', 'mumble') 0.002988643156007173\n",
      "('witch', 'witch') 0.002988643156007173\n",
      "('burn', 'burn') 0.002839210998206814\n",
      "('squeak', 'squeak') 0.002839210998206814\n",
      "('ni', 'ni') 0.0026897788404064557\n",
      "('saw', 'saw') 0.0022414823670053796\n",
      "('clap', 'clap') 0.0014943215780035865\n",
      "('heh', 'heh') 0.0014943215780035865\n",
      "('boom', 'boom') 0.0013448894202032278\n",
      "('hee', 'hee') 0.001195457262402869\n",
      "('shh', 'shh') 0.001195457262402869\n",
      "('haw', 'haw') 0.0010460251046025104\n",
      "('arthur', 'arthur') 0.0008965929468021519\n",
      "('music', 'music') 0.0008965929468021519\n",
      "('thank', 'thank') 0.0008965929468021519\n",
      "('hold', 'hold') 0.0007471607890017932\n",
      "('launcelot', 'launcelot') 0.0007471607890017932\n",
      "('oh', 'oh') 0.0007471607890017932\n",
      "('please', 'please') 0.0007471607890017932\n",
      "('rewr', 'rewr') 0.0007471607890017932\n",
      "('right', 'right') 0.0007471607890017932\n",
      "('stop', 'stop') 0.0007471607890017932\n",
      "('villager', 'villager') 0.0007471607890017932\n",
      "('brave', 'brave') 0.0005977286312014345\n",
      "('giggle', 'giggle') 0.0005977286312014345\n",
      "('pound', 'pound') 0.0005977286312014345\n",
      "('robin', 'robin') 0.0005977286312014345\n",
      "('come', 'come') 0.00044829647340107593\n",
      "('concorde', 'concorde') 0.00044829647340107593\n",
      "('ecky', 'ecky') 0.00044829647340107593\n",
      "('g', 'g') 0.00044829647340107593\n",
      "('j', 'j') 0.00044829647340107593\n",
      "('quiet', 'quiet') 0.00044829647340107593\n",
      "('sorry', 'sorry') 0.00044829647340107593\n",
      "('uh', 'uh') 0.00044829647340107593\n",
      "('away', 'away') 0.00029886431560071725\n",
      "('cave', 'cave') 0.00029886431560071725\n",
      "('cough', 'cough') 0.00029886431560071725\n",
      "('father', 'father') 0.00029886431560071725\n",
      "('galahad', 'galahad') 0.00029886431560071725\n",
      "('grail', 'grail') 0.00029886431560071725\n",
      "('guard', 'guard') 0.00029886431560071725\n",
      "('hang', 'hang') 0.00029886431560071725\n",
      "('n', 'n') 0.00029886431560071725\n",
      "('ow', 'ow') 0.00029886431560071725\n",
      "('path', 'path') 0.00029886431560071725\n",
      "('quack', 'quack') 0.00029886431560071725\n",
      "('rrrr', 'rrrr') 0.00029886431560071725\n",
      "('scribble', 'scribble') 0.00029886431560071725\n",
      "('spanking', 'spanking') 0.00029886431560071725\n",
      "('thud', 'thud') 0.00029886431560071725\n",
      "('tim', 'tim') 0.00029886431560071725\n",
      "('yes', 'yes') 0.00029886431560071725\n",
      "('zoot', 'zoot') 0.00029886431560071725\n",
      "('aaaugh', 'aaaugh') 0.00014943215780035862\n",
      "('ahh', 'ahh') 0.00014943215780035862\n",
      "('always', 'always') 0.00014943215780035862\n",
      "('b', 'b') 0.00014943215780035862\n",
      "('bad', 'bad') 0.00014943215780035862\n",
      "('bedevere', 'bedevere') 0.00014943215780035862\n",
      "('blessing', 'blessing') 0.00014943215780035862\n",
      "('churches', 'churches') 0.00014943215780035862\n",
      "('cut', 'cut') 0.00014943215780035862\n",
      "('dennis', 'dennis') 0.00014943215780035862\n",
      "('enchanter', 'enchanter') 0.00014943215780035862\n",
      "('every', 'every') 0.00014943215780035862\n",
      "('floats', 'floats') 0.00014943215780035862\n",
      "('fold', 'fold') 0.00014943215780035862\n",
      "('follow', 'follow') 0.00014943215780035862\n",
      "('go', 'go') 0.00014943215780035862\n",
      "('hear', 'hear') 0.00014943215780035862\n",
      "('help', 'help') 0.00014943215780035862\n",
      "('herbert', 'herbert') 0.00014943215780035862\n",
      "('historian', 'historian') 0.00014943215780035862\n",
      "('hmm', 'hmm') 0.00014943215780035862\n",
      "('ho', 'ho') 0.00014943215780035862\n",
      "('hoo', 'hoo') 0.00014943215780035862\n",
      "('howl', 'howl') 0.00014943215780035862\n",
      "('knights', 'knights') 0.00014943215780035862\n",
      "('lapin', 'lapin') 0.00014943215780035862\n",
      "('lead', 'lead') 0.00014943215780035862\n",
      "('like', 'like') 0.00014943215780035862\n",
      "('long', 'long') 0.00014943215780035862\n",
      "('nay', 'nay') 0.00014943215780035862\n",
      "('nothing', 'nothing') 0.00014943215780035862\n",
      "('ohh', 'ohh') 0.00014943215780035862\n",
      "('oui', 'oui') 0.00014943215780035862\n",
      "('packing', 'packing') 0.00014943215780035862\n",
      "('quick', 'quick') 0.00014943215780035862\n",
      "('said', 'said') 0.00014943215780035862\n",
      "('say', 'say') 0.00014943215780035862\n",
      "('seen', 'seen') 0.00014943215780035862\n",
      "('shut', 'shut') 0.00014943215780035862\n",
      "('sir', 'sir') 0.00014943215780035862\n",
      "('surprise', 'surprise') 0.00014943215780035862\n",
      "('thppt', 'thppt') 0.00014943215780035862\n",
      "('three', 'three') 0.00014943215780035862\n",
      "('u', 'u') 0.00014943215780035862\n",
      "('whinny', 'whinny') 0.00014943215780035862\n",
      "('wicked', 'wicked') 0.00014943215780035862\n",
      "('word', 'word') 0.00014943215780035862\n",
      "('yay', 'yay') 0.00014943215780035862\n",
      "('yeah', 'yeah') 0.00014943215780035862\n",
      "('yup', 'yup') 0.00014943215780035862\n"
     ]
    }
   ],
   "source": [
    "#Find all bigrams that use same two words\n",
    "monty_consecutive_finder = nltk.BigramCollocationFinder.from_words(Monty_Python_tokens)\n",
    "monty_consecutive_finder.apply_ngram_filter(lambda word1, word2: word1 != word2)\n",
    "knicks_consecutive_finder = nltk.BigramCollocationFinder.from_words(Knicks_History_tokens)\n",
    "knicks_consecutive_finder.apply_ngram_filter(lambda word1, word2: word1 != word2)\n",
    "\n",
    "print('\\nMONTY PYTHON BIGRAMS WITH SAME TWO WORDS')\n",
    "for row in monty_consecutive_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNICKS HISTORY BIGRAMS WITH SAME TWO WORDS\n",
      "('knicks', 'knicks') 0.0004039587962027873\n",
      "('team', 'team') 0.0004039587962027873\n",
      "('hill', 'hill') 0.00020197939810139365\n",
      "('irish', 'irish') 0.00020197939810139365\n",
      "('jackson', 'jackson') 0.00020197939810139365\n",
      "('piece', 'piece') 0.00020197939810139365\n",
      "('rebounds', 'rebounds') 0.00020197939810139365\n",
      "('season', 'season') 0.00020197939810139365\n"
     ]
    }
   ],
   "source": [
    "print('\\nKNICKS HISTORY BIGRAMS WITH SAME TWO WORDS')\n",
    "for row in knicks_consecutive_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It seems there are quite a few of these cases in Monty Python and a few in Knicks History. \n",
      "The removal of stopwords and non-alphabetic characters during tokenization can exlain some of these, but it is still quite peculiar the large number of times this occurs in Monty Python.\n",
      "We will repeat this process, this time using a frequency filter to see if any of these repeating bigrams happen multiple times.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "\n",
    "It seems there are quite a few of these cases in Monty Python and a few in Knicks History. \n",
    "The removal of stopwords and non-alphabetic characters during tokenization can exlain some of these, but it is still quite peculiar the large number of times this occurs in Monty Python.\n",
    "We will repeat this process, this time using a frequency filter to see if any of these repeating bigrams happen multiple times.\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 1\n",
      "('ha', 'ha') 0.00552898983861327\n",
      "('clop', 'clop') 0.0038852361028093247\n",
      "('hello', 'hello') 0.00328750747160789\n",
      "('mumble', 'mumble') 0.002988643156007173\n",
      "('witch', 'witch') 0.002988643156007173\n",
      "('burn', 'burn') 0.002839210998206814\n",
      "('squeak', 'squeak') 0.002839210998206814\n",
      "('ni', 'ni') 0.0026897788404064557\n",
      "('saw', 'saw') 0.0022414823670053796\n",
      "('clap', 'clap') 0.0014943215780035865\n",
      "('heh', 'heh') 0.0014943215780035865\n",
      "('boom', 'boom') 0.0013448894202032278\n",
      "('hee', 'hee') 0.001195457262402869\n",
      "('shh', 'shh') 0.001195457262402869\n",
      "('haw', 'haw') 0.0010460251046025104\n",
      "('arthur', 'arthur') 0.0008965929468021519\n",
      "('music', 'music') 0.0008965929468021519\n",
      "('thank', 'thank') 0.0008965929468021519\n",
      "('hold', 'hold') 0.0007471607890017932\n",
      "('launcelot', 'launcelot') 0.0007471607890017932\n",
      "('oh', 'oh') 0.0007471607890017932\n",
      "('please', 'please') 0.0007471607890017932\n",
      "('rewr', 'rewr') 0.0007471607890017932\n",
      "('right', 'right') 0.0007471607890017932\n",
      "('stop', 'stop') 0.0007471607890017932\n",
      "('villager', 'villager') 0.0007471607890017932\n",
      "('brave', 'brave') 0.0005977286312014345\n",
      "('giggle', 'giggle') 0.0005977286312014345\n",
      "('pound', 'pound') 0.0005977286312014345\n",
      "('robin', 'robin') 0.0005977286312014345\n",
      "('come', 'come') 0.00044829647340107593\n",
      "('concorde', 'concorde') 0.00044829647340107593\n",
      "('ecky', 'ecky') 0.00044829647340107593\n",
      "('g', 'g') 0.00044829647340107593\n",
      "('j', 'j') 0.00044829647340107593\n",
      "('quiet', 'quiet') 0.00044829647340107593\n",
      "('sorry', 'sorry') 0.00044829647340107593\n",
      "('uh', 'uh') 0.00044829647340107593\n",
      "('away', 'away') 0.00029886431560071725\n",
      "('cave', 'cave') 0.00029886431560071725\n",
      "('cough', 'cough') 0.00029886431560071725\n",
      "('father', 'father') 0.00029886431560071725\n",
      "('galahad', 'galahad') 0.00029886431560071725\n",
      "('grail', 'grail') 0.00029886431560071725\n",
      "('guard', 'guard') 0.00029886431560071725\n",
      "('hang', 'hang') 0.00029886431560071725\n",
      "('n', 'n') 0.00029886431560071725\n",
      "('ow', 'ow') 0.00029886431560071725\n",
      "('path', 'path') 0.00029886431560071725\n",
      "('quack', 'quack') 0.00029886431560071725\n",
      "('rrrr', 'rrrr') 0.00029886431560071725\n",
      "('scribble', 'scribble') 0.00029886431560071725\n",
      "('spanking', 'spanking') 0.00029886431560071725\n",
      "('thud', 'thud') 0.00029886431560071725\n",
      "('tim', 'tim') 0.00029886431560071725\n",
      "('yes', 'yes') 0.00029886431560071725\n",
      "('zoot', 'zoot') 0.00029886431560071725\n"
     ]
    }
   ],
   "source": [
    "#Find all bigrams that use same two words and frequency > 1\n",
    "monty_consecutive_finder.apply_freq_filter(2)\n",
    "\n",
    "print('\\nMONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 1')\n",
    "for row in monty_consecutive_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNICKS HISTORY BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 1\n",
      "('knicks', 'knicks') 0.0004039587962027873\n",
      "('team', 'team') 0.0004039587962027873\n"
     ]
    }
   ],
   "source": [
    "#Find all bigrams that use same two words and frequency > 1\n",
    "knicks_consecutive_finder.apply_freq_filter(2)\n",
    "\n",
    "print('\\nKNICKS HISTORY BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 1')\n",
    "for row in knicks_consecutive_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Very interesting... what if we crank the freuqnecy filter up to >= 5?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "\n",
    "Very interesting... what if we crank the freuqnecy filter up to >= 5?\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY >= 5\n",
      "('ha', 'ha') 0.00552898983861327\n",
      "('clop', 'clop') 0.0038852361028093247\n",
      "('hello', 'hello') 0.00328750747160789\n",
      "('mumble', 'mumble') 0.002988643156007173\n",
      "('witch', 'witch') 0.002988643156007173\n",
      "('burn', 'burn') 0.002839210998206814\n",
      "('squeak', 'squeak') 0.002839210998206814\n",
      "('ni', 'ni') 0.0026897788404064557\n",
      "('saw', 'saw') 0.0022414823670053796\n",
      "('clap', 'clap') 0.0014943215780035865\n",
      "('heh', 'heh') 0.0014943215780035865\n",
      "('boom', 'boom') 0.0013448894202032278\n",
      "('hee', 'hee') 0.001195457262402869\n",
      "('shh', 'shh') 0.001195457262402869\n",
      "('haw', 'haw') 0.0010460251046025104\n",
      "('arthur', 'arthur') 0.0008965929468021519\n",
      "('music', 'music') 0.0008965929468021519\n",
      "('thank', 'thank') 0.0008965929468021519\n",
      "('hold', 'hold') 0.0007471607890017932\n",
      "('launcelot', 'launcelot') 0.0007471607890017932\n",
      "('oh', 'oh') 0.0007471607890017932\n",
      "('please', 'please') 0.0007471607890017932\n",
      "('rewr', 'rewr') 0.0007471607890017932\n",
      "('right', 'right') 0.0007471607890017932\n",
      "('stop', 'stop') 0.0007471607890017932\n",
      "('villager', 'villager') 0.0007471607890017932\n",
      "\n",
      "KNICKS HISTORY BIGRAMS WITH SAME TWO WORDS AND FREQUENCY >= 5\n"
     ]
    }
   ],
   "source": [
    "#Find all bigrams that use same two words and frequency >= 5\n",
    "monty_consecutive_finder.apply_freq_filter(5)\n",
    "\n",
    "print('\\nMONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY >= 5')\n",
    "for row in monty_consecutive_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])\n",
    "\n",
    "#Find all bigrams that use same two words and frequency >= 5\n",
    "knicks_consecutive_finder.apply_freq_filter(5)\n",
    "\n",
    "print('\\nKNICKS HISTORY BIGRAMS WITH SAME TWO WORDS AND FREQUENCY >= 5')\n",
    "for row in knicks_consecutive_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Well... we have shown that bigrams with the same two words occur much more often in Monty Python, to say the least. But what is going on here? Where does it end? \n",
      "Let's see if any of the Monty Python bigrams with the same two words occur more than 10 times.\n",
      "\n",
      "\n",
      "MONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 10\n",
      "('ha', 'ha') 0.00552898983861327\n",
      "('clop', 'clop') 0.0038852361028093247\n",
      "('hello', 'hello') 0.00328750747160789\n",
      "('mumble', 'mumble') 0.002988643156007173\n",
      "('witch', 'witch') 0.002988643156007173\n",
      "('burn', 'burn') 0.002839210998206814\n",
      "('squeak', 'squeak') 0.002839210998206814\n",
      "('ni', 'ni') 0.0026897788404064557\n",
      "('saw', 'saw') 0.0022414823670053796\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "\n",
    "Well... we have shown that bigrams with the same two words occur much more often in Monty Python, to say the least. But what is going on here? Where does it end? \n",
    "Let\\'s see if any of the Monty Python bigrams with the same two words occur more than 10 times.\n",
    "''')\n",
    "\n",
    "monty_consecutive_finder.apply_freq_filter(11)\n",
    "\n",
    "print('\\nMONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 10')\n",
    "for row in monty_consecutive_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall that ('ha', 'ha') was the most frequent bigram in all bigrams of Monty Python, so these results, while strange, are on par with our findings thus far.\n",
      "Let's keep increasing the frquency until we see relatively how many times ('ha', 'ha') occurs.\n",
      "\n",
      "MONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 20\n",
      "('ha', 'ha') 0.00552898983861327\n",
      "('clop', 'clop') 0.0038852361028093247\n",
      "('hello', 'hello') 0.00328750747160789\n",
      "\n",
      "MONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 35\n",
      "('ha', 'ha') 0.00552898983861327\n",
      "\n",
      "MONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 50\n",
      "\n",
      "\n",
      "We see here that ('ha', 'ha') occurs somewhere between 36 and 50 times.\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "\n",
    "Recall that (\\'ha\\', \\'ha\\') was the most frequent bigram in all bigrams of Monty Python, so these results, while strange, are on par with our findings thus far.\n",
    "Let\\'s keep increasing the frquency until we see relatively how many times (\\'ha\\', \\'ha\\') occurs.\n",
    "''')\n",
    "\n",
    "monty_consecutive_finder.apply_freq_filter(21)\n",
    "print('MONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 20')\n",
    "for row in monty_consecutive_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])\n",
    "\n",
    "monty_consecutive_finder.apply_freq_filter(36)\n",
    "print('\\nMONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 35')\n",
    "for row in monty_consecutive_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])\n",
    "\n",
    "monty_consecutive_finder.apply_freq_filter(51)\n",
    "print('\\nMONTY PYTHON BIGRAMS WITH SAME TWO WORDS AND FREQUENCY > 50')\n",
    "for row in monty_consecutive_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])\n",
    "\n",
    "print('\\n\\nWe see here that (\\'ha\\', \\'ha\\') occurs somewhere between 36 and 50 times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "One final investigation will be looking at trigrams to see if any of them that occur in either Monty Python or Knicks History have the same word three consecutive times.\n",
      "Due to the many bigrams of this nature in Monty Python, I would expect there to be cases of these trigrams in Monty Python, but I do not expect any from Knicks History.\n",
      "But, enough with the speculation, let's see for ourselves!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "\n",
    "One final investigation will be looking at trigrams to see if any of them that occur in either Monty Python or Knicks History have the same word three consecutive times.\n",
    "Due to the many bigrams of this nature in Monty Python, I would expect there to be cases of these trigrams in Monty Python, but I do not expect any from Knicks History.\n",
    "But, enough with the speculation, let\\'s see for ourselves!\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MONTY PYTHON TRIGRAMS WITH SAME THREE WORDS\n",
      "('hello', 'hello', 'hello') 0.0031380753138075313\n",
      "('ha', 'ha', 'ha') 0.002839210998206814\n",
      "('squeak', 'squeak', 'squeak') 0.0022414823670053796\n",
      "('saw', 'saw', 'saw') 0.0020920502092050207\n",
      "('burn', 'burn', 'burn') 0.0019426180514046623\n",
      "('clop', 'clop', 'clop') 0.0019426180514046623\n",
      "('ni', 'ni', 'ni') 0.0017931858936043037\n",
      "('witch', 'witch', 'witch') 0.001643753735803945\n",
      "('mumble', 'mumble', 'mumble') 0.0014943215780035865\n",
      "('shh', 'shh', 'shh') 0.0008965929468021519\n",
      "('boom', 'boom', 'boom') 0.0007471607890017932\n",
      "('clap', 'clap', 'clap') 0.0007471607890017932\n",
      "('haw', 'haw', 'haw') 0.0007471607890017932\n",
      "('hee', 'hee', 'hee') 0.0007471607890017932\n",
      "('heh', 'heh', 'heh') 0.0007471607890017932\n",
      "('rewr', 'rewr', 'rewr') 0.0005977286312014345\n",
      "('thank', 'thank', 'thank') 0.0005977286312014345\n",
      "('hold', 'hold', 'hold') 0.00044829647340107593\n",
      "('brave', 'brave', 'brave') 0.00029886431560071725\n",
      "('ecky', 'ecky', 'ecky') 0.00029886431560071725\n",
      "('g', 'g', 'g') 0.00029886431560071725\n",
      "('giggle', 'giggle', 'giggle') 0.00029886431560071725\n",
      "('pound', 'pound', 'pound') 0.00029886431560071725\n",
      "('quiet', 'quiet', 'quiet') 0.00029886431560071725\n",
      "('stop', 'stop', 'stop') 0.00029886431560071725\n",
      "('villager', 'villager', 'villager') 0.00029886431560071725\n",
      "('hang', 'hang', 'hang') 0.00014943215780035862\n",
      "('j', 'j', 'j') 0.00014943215780035862\n",
      "('launcelot', 'launcelot', 'launcelot') 0.00014943215780035862\n",
      "('n', 'n', 'n') 0.00014943215780035862\n",
      "('ow', 'ow', 'ow') 0.00014943215780035862\n",
      "('path', 'path', 'path') 0.00014943215780035862\n",
      "('please', 'please', 'please') 0.00014943215780035862\n",
      "('quack', 'quack', 'quack') 0.00014943215780035862\n",
      "('right', 'right', 'right') 0.00014943215780035862\n",
      "('rrrr', 'rrrr', 'rrrr') 0.00014943215780035862\n",
      "('scribble', 'scribble', 'scribble') 0.00014943215780035862\n",
      "('sorry', 'sorry', 'sorry') 0.00014943215780035862\n",
      "('thud', 'thud', 'thud') 0.00014943215780035862\n",
      "('uh', 'uh', 'uh') 0.00014943215780035862\n",
      "\n",
      "KNICKS HISTORY TRIGRAMS WITH SAME THREE WORDS\n"
     ]
    }
   ],
   "source": [
    "#Find all trigrams that use same three words\n",
    "monty_consecutive_trigram_finder = nltk.TrigramCollocationFinder.from_words(Monty_Python_tokens)\n",
    "monty_consecutive_trigram_finder.apply_ngram_filter(lambda word1, word2, word3: not (word1 == word2 and word2 == word3))\n",
    "knicks_consecutive_trigram_finder = nltk.TrigramCollocationFinder.from_words(Knicks_History_tokens)\n",
    "knicks_consecutive_trigram_finder.apply_ngram_filter(lambda word1, word2, word3: not (word1 == word2 and word2 == word3))\n",
    "\n",
    "print('\\nMONTY PYTHON TRIGRAMS WITH SAME THREE WORDS')\n",
    "for row in monty_consecutive_trigram_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])\n",
    "\n",
    "print('\\nKNICKS HISTORY TRIGRAMS WITH SAME THREE WORDS')\n",
    "for row in knicks_consecutive_trigram_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Very interesting results... We will finish up by adding frequency filters to the Monty Python trigrams as we did with the bigrams.\n",
      "\n",
      "MONTY PYTHON TRIGRAMS WITH SAME THREE WORDS AND FREQUENCY > 1\n",
      "('hello', 'hello', 'hello') 0.0031380753138075313\n",
      "('ha', 'ha', 'ha') 0.002839210998206814\n",
      "('squeak', 'squeak', 'squeak') 0.0022414823670053796\n",
      "('saw', 'saw', 'saw') 0.0020920502092050207\n",
      "('burn', 'burn', 'burn') 0.0019426180514046623\n",
      "('clop', 'clop', 'clop') 0.0019426180514046623\n",
      "('ni', 'ni', 'ni') 0.0017931858936043037\n",
      "('witch', 'witch', 'witch') 0.001643753735803945\n",
      "('mumble', 'mumble', 'mumble') 0.0014943215780035865\n",
      "('shh', 'shh', 'shh') 0.0008965929468021519\n",
      "('boom', 'boom', 'boom') 0.0007471607890017932\n",
      "('clap', 'clap', 'clap') 0.0007471607890017932\n",
      "('haw', 'haw', 'haw') 0.0007471607890017932\n",
      "('hee', 'hee', 'hee') 0.0007471607890017932\n",
      "('heh', 'heh', 'heh') 0.0007471607890017932\n",
      "('rewr', 'rewr', 'rewr') 0.0005977286312014345\n",
      "('thank', 'thank', 'thank') 0.0005977286312014345\n",
      "('hold', 'hold', 'hold') 0.00044829647340107593\n",
      "('brave', 'brave', 'brave') 0.00029886431560071725\n",
      "('ecky', 'ecky', 'ecky') 0.00029886431560071725\n",
      "('g', 'g', 'g') 0.00029886431560071725\n",
      "('giggle', 'giggle', 'giggle') 0.00029886431560071725\n",
      "('pound', 'pound', 'pound') 0.00029886431560071725\n",
      "('quiet', 'quiet', 'quiet') 0.00029886431560071725\n",
      "('stop', 'stop', 'stop') 0.00029886431560071725\n",
      "('villager', 'villager', 'villager') 0.00029886431560071725\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "\n",
    "Very interesting results... We will finish up by adding frequency filters to the Monty Python trigrams as we did with the bigrams.\n",
    "''')\n",
    "\n",
    "monty_consecutive_trigram_finder.apply_freq_filter(2)\n",
    "print('MONTY PYTHON TRIGRAMS WITH SAME THREE WORDS AND FREQUENCY > 1')\n",
    "for row in monty_consecutive_trigram_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MONTY PYTHON TRIGRAMS WITH SAME THREE WORDS AND FREQUENCY > 5\n",
      "('hello', 'hello', 'hello') 0.0031380753138075313\n",
      "('ha', 'ha', 'ha') 0.002839210998206814\n",
      "('squeak', 'squeak', 'squeak') 0.0022414823670053796\n",
      "('saw', 'saw', 'saw') 0.0020920502092050207\n",
      "('burn', 'burn', 'burn') 0.0019426180514046623\n",
      "('clop', 'clop', 'clop') 0.0019426180514046623\n",
      "('ni', 'ni', 'ni') 0.0017931858936043037\n",
      "('witch', 'witch', 'witch') 0.001643753735803945\n",
      "('mumble', 'mumble', 'mumble') 0.0014943215780035865\n",
      "('shh', 'shh', 'shh') 0.0008965929468021519\n",
      "\n",
      "MONTY PYTHON TRIGRAMS WITH SAME THREE WORDS AND FREQUENCY > 10\n",
      "('hello', 'hello', 'hello') 0.0031380753138075313\n",
      "('ha', 'ha', 'ha') 0.002839210998206814\n",
      "('squeak', 'squeak', 'squeak') 0.0022414823670053796\n",
      "('saw', 'saw', 'saw') 0.0020920502092050207\n",
      "('burn', 'burn', 'burn') 0.0019426180514046623\n",
      "('clop', 'clop', 'clop') 0.0019426180514046623\n",
      "('ni', 'ni', 'ni') 0.0017931858936043037\n",
      "('witch', 'witch', 'witch') 0.001643753735803945\n",
      "\n",
      "MONTY PYTHON TRIGRAMS WITH SAME THREE WORDS AND FREQUENCY > 20\n",
      "('hello', 'hello', 'hello') 0.0031380753138075313\n",
      "\n",
      "MONTY PYTHON TRIGRAMS WITH SAME THREE WORDS AND FREQUENCY > 30\n",
      "\n",
      "\n",
      "\n",
      "The most frequent trigram with all 3 words the same in Monty Python, ('hello', 'hello', 'hello'), occurs somewhere between 21 and 30 times.\n"
     ]
    }
   ],
   "source": [
    "monty_consecutive_trigram_finder.apply_freq_filter(6)\n",
    "print('\\nMONTY PYTHON TRIGRAMS WITH SAME THREE WORDS AND FREQUENCY > 5')\n",
    "for row in monty_consecutive_trigram_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])\n",
    "\n",
    "monty_consecutive_trigram_finder.apply_freq_filter(11)\n",
    "print('\\nMONTY PYTHON TRIGRAMS WITH SAME THREE WORDS AND FREQUENCY > 10')\n",
    "for row in monty_consecutive_trigram_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])\n",
    "\n",
    "monty_consecutive_trigram_finder.apply_freq_filter(21)\n",
    "print('\\nMONTY PYTHON TRIGRAMS WITH SAME THREE WORDS AND FREQUENCY > 20')\n",
    "for row in monty_consecutive_trigram_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])\n",
    "\n",
    "monty_consecutive_trigram_finder.apply_freq_filter(31)\n",
    "print('\\nMONTY PYTHON TRIGRAMS WITH SAME THREE WORDS AND FREQUENCY > 30')\n",
    "for row in monty_consecutive_trigram_finder.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq): print(row[0], row[1])\n",
    "\n",
    "print('\\n\\n\\nThe most frequent trigram with all 3 words the same in Monty Python, (\\'hello\\', \\'hello\\', \\'hello\\'), occurs somewhere between 21 and 30 times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONCLUSION:\n",
      "\n",
      "Both documents, Monty Python and Knicks History, had some pretty valubale word tokens and bigrams in terms of giving a bit of context without needing to read the actual documents.\n",
      "However, Monty Python seems to have a very large amount of bigrams, and even trigrams, with the same words used consecutively.\n",
      "This is a bit peculiar, especially for someone that is not too familar with Monty Python, but there is also a chance that the removal of stopwords and non-alphabetic tokens made this\n",
      "pehnomenon seem to be more frequent than it actually occurs. Either way, it certianly could help to infer a bit about the lack of seriousness in Monty Python\n",
      "and gives a hint that it could be comedy, even to someone who is unaware that Monty Python is a comedy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "CONCLUSION:\n",
    "\n",
    "Both documents, Monty Python and Knicks History, had some pretty valubale word tokens and bigrams in terms of giving a bit of context without needing to read the actual documents.\n",
    "However, Monty Python seems to have a very large amount of bigrams, and even trigrams, with the same words used consecutively.\n",
    "This is a bit peculiar, especially for someone that is not too familar with Monty Python, but there is also a chance that the removal of stopwords and non-alphabetic tokens made this\n",
    "pehnomenon seem to be more frequent than it actually occurs. Either way, it certianly could help to infer a bit about the lack of seriousness in Monty Python\n",
    "and gives a hint that it could be comedy, even to someone who is unaware that Monty Python is a comedy.\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JUPYTER",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
