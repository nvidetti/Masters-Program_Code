{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nick Videtti\n",
    "<br>\n",
    "IST-664 Natural Language Processing\n",
    "<br>\n",
    "Final Project\n",
    "<br>\n",
    "Winter 2023\n",
    "<br>\n",
    "<br>\n",
    "This project does analysis on both the words and the sentences in the Brown corpus of NLTK.\n",
    "<br>\n",
    "This analysis will include corpus statistics, predicting category of words and sentences using NLTK Naive Bayes Classifier, and other experiments.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "The first step is to pull the words and sentences from the Brown corpus and put them in a pandas DataFrame with their corresponding categories. We will remove non-alphabetic phrases from both the words and sentences, and will remove NLTK's English stopwords from the words.\n",
    "<br>\n",
    "Due to processing time, stopwords will not be removed in the sentences, but this issue will be addressed later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas\n",
    "import re\n",
    "\n",
    "#Create list of catgories to loop through\n",
    "categories = nltk.corpus.brown.categories()\n",
    "\n",
    "#Create empty dataframes to store categorized sentences/words\n",
    "SENTENCE_CATEGORIES = pandas.DataFrame(columns = ['Sentence','Category'])\n",
    "WORD_CATEGORIES = pandas.DataFrame(columns = ['Word','Category'])\n",
    "\n",
    "#Loop through categories\n",
    "for category in categories:\n",
    "    #Create sentnece dataframe for each category and add it to SENTENCE_CATEGORIES\n",
    "    sents_og = nltk.corpus.brown.sents(categories = categories[categories.index(category)])\n",
    "    sents_category = pandas.DataFrame([['',category,'',sent_og] for sent_og in sents_og], columns = ['Sentence', 'Category', 'Readable Sentence', 'Original Sentence'])\n",
    "    SENTENCE_CATEGORIES = pandas.concat([SENTENCE_CATEGORIES, sents_category])\n",
    "\n",
    "    #Create word dataframe for each category and add it to WORD_CATEGORIES\n",
    "    words = nltk.corpus.brown.words(categories = categories[categories.index(category)])\n",
    "    #Remove non-alphabetic phrases from words and convert to lowercase\n",
    "    words = [word.lower() for word in words if re.compile('^[a-z]+$').match(word.lower())]\n",
    "    words_category = pandas.DataFrame([[word,category] for word in words], columns = ['Word', 'Category'])\n",
    "    WORD_CATEGORIES = pandas.concat([WORD_CATEGORIES, words_category])\n",
    "\n",
    "#Remove Stopwords from WORD_CATEGORIES\n",
    "WORD_CATEGORIES = WORD_CATEGORIES[~WORD_CATEGORIES['Word'].isin(nltk.corpus.stopwords.words('english'))]\n",
    "\n",
    "#Make columns for \"Readable Sentence\" and \"Sentence\"\n",
    "SENTENCE_CATEGORIES['Sentence'] = [[word.lower() for word in sent if re.compile('^[aA-zZ]+$').match(word.lower())] for sent in SENTENCE_CATEGORIES['Original Sentence']]\n",
    "SENTENCE_CATEGORIES['Readable Sentence'] = SENTENCE_CATEGORIES['Sentence'].str.join(' ')\n",
    "#Fix \"Original Sentence\" column\n",
    "SENTENCE_CATEGORIES['Original Sentence'] = SENTENCE_CATEGORIES['Original Sentence'].str.join(' ').str.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the first 10 rows of the pandas DataFrame we created for the sentences. The main setence we will use is in list format, so another column was created for \"Readable Sentence\", which simply concatenates the Sentence list with spaces in between, and \"Original Sentence\", which shows the original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Category</th>\n",
       "      <th>Readable Sentence</th>\n",
       "      <th>Original Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[dan, morgan, told, himself, he, would, forget...</td>\n",
       "      <td>adventure</td>\n",
       "      <td>dan morgan told himself he would forget ann tu...</td>\n",
       "      <td>Dan Morgan told himself he would forget Ann Tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[he, was, well, rid, of, her]</td>\n",
       "      <td>adventure</td>\n",
       "      <td>he was well rid of her</td>\n",
       "      <td>He was well rid of her .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[he, certainly, want, a, wife, who, was, fickl...</td>\n",
       "      <td>adventure</td>\n",
       "      <td>he certainly want a wife who was fickle as ann</td>\n",
       "      <td>He certainly didn't want a wife who was fickle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[if, he, had, married, her, have, been, asking...</td>\n",
       "      <td>adventure</td>\n",
       "      <td>if he had married her have been asking for tro...</td>\n",
       "      <td>If he had married her , he'd have been asking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[but, all, of, this, was, rationalization]</td>\n",
       "      <td>adventure</td>\n",
       "      <td>but all of this was rationalization</td>\n",
       "      <td>But all of this was rationalization .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[sometimes, he, woke, up, in, the, middle, of,...</td>\n",
       "      <td>adventure</td>\n",
       "      <td>sometimes he woke up in the middle of the nigh...</td>\n",
       "      <td>Sometimes he woke up in the middle of the nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[his, plans, and, dreams, had, revolved, aroun...</td>\n",
       "      <td>adventure</td>\n",
       "      <td>his plans and dreams had revolved around her s...</td>\n",
       "      <td>His plans and dreams had revolved around her s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[the, easiest, thing, would, be, to, sell, out...</td>\n",
       "      <td>adventure</td>\n",
       "      <td>the easiest thing would be to sell out to al b...</td>\n",
       "      <td>The easiest thing would be to sell out to Al B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[the, best, antidote, for, the, bitterness, an...</td>\n",
       "      <td>adventure</td>\n",
       "      <td>the best antidote for the bitterness and disap...</td>\n",
       "      <td>The best antidote for the bitterness and disap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[he, found, that, if, he, was, tired, enough, ...</td>\n",
       "      <td>adventure</td>\n",
       "      <td>he found that if he was tired enough at night ...</td>\n",
       "      <td>He found that if he was tired enough at night ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence   Category  \\\n",
       "0  [dan, morgan, told, himself, he, would, forget...  adventure   \n",
       "1                      [he, was, well, rid, of, her]  adventure   \n",
       "2  [he, certainly, want, a, wife, who, was, fickl...  adventure   \n",
       "3  [if, he, had, married, her, have, been, asking...  adventure   \n",
       "4         [but, all, of, this, was, rationalization]  adventure   \n",
       "5  [sometimes, he, woke, up, in, the, middle, of,...  adventure   \n",
       "6  [his, plans, and, dreams, had, revolved, aroun...  adventure   \n",
       "7  [the, easiest, thing, would, be, to, sell, out...  adventure   \n",
       "8  [the, best, antidote, for, the, bitterness, an...  adventure   \n",
       "9  [he, found, that, if, he, was, tired, enough, ...  adventure   \n",
       "\n",
       "                                   Readable Sentence  \\\n",
       "0  dan morgan told himself he would forget ann tu...   \n",
       "1                             he was well rid of her   \n",
       "2     he certainly want a wife who was fickle as ann   \n",
       "3  if he had married her have been asking for tro...   \n",
       "4                but all of this was rationalization   \n",
       "5  sometimes he woke up in the middle of the nigh...   \n",
       "6  his plans and dreams had revolved around her s...   \n",
       "7  the easiest thing would be to sell out to al b...   \n",
       "8  the best antidote for the bitterness and disap...   \n",
       "9  he found that if he was tired enough at night ...   \n",
       "\n",
       "                                   Original Sentence  \n",
       "0  Dan Morgan told himself he would forget Ann Tu...  \n",
       "1                           He was well rid of her .  \n",
       "2  He certainly didn't want a wife who was fickle...  \n",
       "3  If he had married her , he'd have been asking ...  \n",
       "4              But all of this was rationalization .  \n",
       "5  Sometimes he woke up in the middle of the nigh...  \n",
       "6  His plans and dreams had revolved around her s...  \n",
       "7  The easiest thing would be to sell out to Al B...  \n",
       "8  The best antidote for the bitterness and disap...  \n",
       "9  He found that if he was tired enough at night ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect first 10 rows of Sentence data\n",
    "SENTENCE_CATEGORIES.iloc[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the first 10 rows of the pandas DataFrame we created for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dan</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>morgan</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>told</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>would</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>forget</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ann</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>turner</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>well</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rid</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>certainly</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word   Category\n",
       "0         dan  adventure\n",
       "1      morgan  adventure\n",
       "2        told  adventure\n",
       "5       would  adventure\n",
       "6      forget  adventure\n",
       "7         ann  adventure\n",
       "8      turner  adventure\n",
       "11       well  adventure\n",
       "12        rid  adventure\n",
       "16  certainly  adventure"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect first 10 rows of Word data\n",
    "WORD_CATEGORIES.iloc[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create our word features for our text classifiers! Let's create those then take a look at the first few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'Word': 'forget'}, 'adventure'),\n",
       " ({'Word': 'married'}, 'adventure'),\n",
       " ({'Word': 'asking'}, 'adventure'),\n",
       " ({'Word': 'could'}, 'adventure'),\n",
       " ({'Word': 'long'}, 'adventure'),\n",
       " ({'Word': 'went'}, 'adventure'),\n",
       " ({'Word': 'found'}, 'adventure'),\n",
       " ({'Word': 'less'}, 'adventure'),\n",
       " ({'Word': 'plenty'}, 'adventure'),\n",
       " ({'Word': 'could'}, 'adventure'),\n",
       " ({'Word': 'water'}, 'adventure'),\n",
       " ({'Word': 'much'}, 'adventure'),\n",
       " ({'Word': 'would'}, 'adventure'),\n",
       " ({'Word': 'give'}, 'adventure'),\n",
       " ({'Word': 'asleep'}, 'adventure'),\n",
       " ({'Word': 'happened'}, 'adventure'),\n",
       " ({'Word': 'last'}, 'adventure'),\n",
       " ({'Word': 'people'}, 'adventure'),\n",
       " ({'Word': 'could'}, 'adventure'),\n",
       " ({'Word': 'least'}, 'adventure')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create word features\n",
    "top_words = [word for (word, freq) in nltk.FreqDist([word for word in words]).most_common(1000)]\n",
    "top_words_cats = WORD_CATEGORIES[WORD_CATEGORIES['Word'].isin(top_words)]\n",
    "word_features = [({'Word': top_words_cats.iloc[row]['Word']}, top_words_cats.iloc[row]['Category']) for row in top_words_cats.index]\n",
    "\n",
    "word_features[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to create the sentence features! Due to the poor processing time of removing stopwords earlier, we will address the issue here. The features will include first word of sentences with category, last word of sentences with category, and sentence length with category. The first and last words will really be the first and last non-stopwords in the sentences, but the sentence lengths will include stopwords due to the aforementioned processing time issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'First Word': 'morgan'}, 'adventure')\n",
      "({'Last Word': 'told'}, 'adventure')\n",
      "({'Sentence Length': 9}, 'adventure')\n",
      "({'First Word': 'told'}, 'adventure')\n",
      "({'Last Word': 'would'}, 'adventure')\n",
      "({'Sentence Length': 6}, 'adventure')\n",
      "({'First Word': 'would'}, 'adventure')\n",
      "({'Last Word': 'well'}, 'adventure')\n",
      "({'Sentence Length': 10}, 'adventure')\n",
      "({'First Word': 'forget'}, 'adventure')\n",
      "({'Last Word': 'want'}, 'adventure')\n",
      "({'Sentence Length': 10}, 'adventure')\n",
      "({'First Word': 'well'}, 'adventure')\n",
      "({'Last Word': 'wife'}, 'adventure')\n",
      "({'Sentence Length': 6}, 'adventure')\n",
      "({'First Word': 'face'}, 'science_fiction')\n",
      "({'Last Word': 'could'}, 'science_fiction')\n",
      "({'Sentence Length': 10}, 'science_fiction')\n",
      "({'First Word': 'could'}, 'science_fiction')\n",
      "({'Last Word': 'talk'}, 'science_fiction')\n",
      "({'Sentence Length': 35}, 'science_fiction')\n",
      "({'First Word': 'talk'}, 'science_fiction')\n",
      "({'Last Word': 'write'}, 'science_fiction')\n",
      "({'Sentence Length': 14}, 'science_fiction')\n",
      "({'First Word': 'write'}, 'science_fiction')\n",
      "({'Last Word': 'get'}, 'science_fiction')\n",
      "({'Sentence Length': 16}, 'science_fiction')\n",
      "({'First Word': 'get'}, 'science_fiction')\n",
      "({'Last Word': 'pain'}, 'science_fiction')\n",
      "({'Sentence Length': 14}, 'science_fiction')\n"
     ]
    }
   ],
   "source": [
    "#Create sentence features for first word of sentence and last word of sentence\n",
    "\n",
    "#First words\n",
    "first_words = []\n",
    "for sent in SENTENCE_CATEGORIES['Sentence']:\n",
    "    for word in sent:\n",
    "        if word not in nltk.corpus.stopwords.words('english'):\n",
    "            first_words.append(word)\n",
    "            break       \n",
    "\n",
    "top_first_words = [word for (word, freq) in nltk.FreqDist([word for word in first_words]).most_common(1000)]\n",
    "top_first_words_cats = WORD_CATEGORIES[WORD_CATEGORIES['Word'].isin(top_first_words)]\n",
    "first_word_features = [({'First Word': top_first_words_cats.iloc[row]['Word']}, top_first_words_cats.iloc[row]['Category']) for row in range(len(top_first_words_cats.index))]\n",
    "\n",
    "#Last words\n",
    "last_words = []\n",
    "for sent in SENTENCE_CATEGORIES['Sentence']:\n",
    "    for word in reversed(sent):\n",
    "        if word not in nltk.corpus.stopwords.words('english'):\n",
    "            last_words.append(word)\n",
    "            break\n",
    "        \n",
    "top_last_words = [word for (word, freq) in nltk.FreqDist([word for word in last_words]).most_common(1000)]\n",
    "top_last_words_cats = WORD_CATEGORIES[WORD_CATEGORIES['Word'].isin(top_last_words)]\n",
    "last_word_features = [({'Last Word': top_last_words_cats.iloc[row]['Word']}, top_last_words_cats.iloc[row]['Category']) for row in range(len(top_last_words_cats.index))]\n",
    "\n",
    "#Sentence lengths\n",
    "sent_length_features = [({'Sentence Length': len(SENTENCE_CATEGORIES.iloc[row]['Sentence'])}, SENTENCE_CATEGORIES.iloc[row]['Category']) for row in range(len(SENTENCE_CATEGORIES.index))]\n",
    "\n",
    "\n",
    "#Look at first 5 and last 5 features for all feature sets\n",
    "for i in list(range(5)) + list(range(-5,0)): print(first_word_features[i]); print(last_word_features[i]); print(sent_length_features[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the features in our classifiers, but first let's put all of our results into a pandas DataFrame and display the first few rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Category</th>\n",
       "      <th>Actual Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word</td>\n",
       "      <td>forget</td>\n",
       "      <td>adventure</td>\n",
       "      <td>({'Word': 'forget'}, adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word</td>\n",
       "      <td>married</td>\n",
       "      <td>adventure</td>\n",
       "      <td>({'Word': 'married'}, adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word</td>\n",
       "      <td>asking</td>\n",
       "      <td>adventure</td>\n",
       "      <td>({'Word': 'asking'}, adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word</td>\n",
       "      <td>could</td>\n",
       "      <td>adventure</td>\n",
       "      <td>({'Word': 'could'}, adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Word</td>\n",
       "      <td>long</td>\n",
       "      <td>adventure</td>\n",
       "      <td>({'Word': 'long'}, adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Word</td>\n",
       "      <td>went</td>\n",
       "      <td>adventure</td>\n",
       "      <td>({'Word': 'went'}, adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Word</td>\n",
       "      <td>found</td>\n",
       "      <td>adventure</td>\n",
       "      <td>({'Word': 'found'}, adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Word</td>\n",
       "      <td>less</td>\n",
       "      <td>adventure</td>\n",
       "      <td>({'Word': 'less'}, adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Word</td>\n",
       "      <td>plenty</td>\n",
       "      <td>adventure</td>\n",
       "      <td>({'Word': 'plenty'}, adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Word</td>\n",
       "      <td>could</td>\n",
       "      <td>adventure</td>\n",
       "      <td>({'Word': 'could'}, adventure)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value   Category                    Actual Feature\n",
       "0         Word        forget  adventure   ({'Word': 'forget'}, adventure)\n",
       "1         Word       married  adventure  ({'Word': 'married'}, adventure)\n",
       "2         Word        asking  adventure   ({'Word': 'asking'}, adventure)\n",
       "3         Word         could  adventure    ({'Word': 'could'}, adventure)\n",
       "4         Word          long  adventure     ({'Word': 'long'}, adventure)\n",
       "5         Word          went  adventure     ({'Word': 'went'}, adventure)\n",
       "6         Word         found  adventure    ({'Word': 'found'}, adventure)\n",
       "7         Word          less  adventure     ({'Word': 'less'}, adventure)\n",
       "8         Word        plenty  adventure   ({'Word': 'plenty'}, adventure)\n",
       "9         Word         could  adventure    ({'Word': 'could'}, adventure)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create pandas DataFrame for features\n",
    "featuresDF = pandas.DataFrame(columns = ['Feature Name', 'Feature Value', 'Category', 'Actual Feature'])\n",
    "\n",
    "#Assign raw features to a column\n",
    "featuresDF['Actual Feature'] = word_features + first_word_features + last_word_features + sent_length_features\n",
    "\n",
    "#Calculate other columns\n",
    "featuresDF['Feature Name'] = [featuresDF.iloc[row]['Actual Feature'][0].keys() for row in featuresDF.index]\n",
    "featuresDF['Feature Name'] = featuresDF['Feature Name'].str.join('')\n",
    "featuresDF['Feature Value'] = [featuresDF.iloc[row]['Actual Feature'][0].values() for row in featuresDF.index]\n",
    "featuresDF['Feature Value'] = featuresDF['Feature Value'].str.join('')\n",
    "featuresDF['Category'] = [featuresDF.iloc[row]['Actual Feature'][1] for row in featuresDF.index]\n",
    "\n",
    "#Display first 10 rows\n",
    "featuresDF[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to put those features to use! We will now create Naive Bayes Classifiers using NLTK for word features, first word of sentence features, last word of sentence features, sentence length features, and all features. For each of these classifiers, we will also perform a five-fold, ten-fold, and 30-fold cross validation to test their accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Features Classifier\n",
      "RESULTS OF 5-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 36.37 %\n",
      "Minimum Accuracy - 36.01 %\n",
      "Maximum Accuracy - 36.65 %\n",
      "\n",
      "RESULTS OF 10-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 36.41 %\n",
      "Minimum Accuracy - 35.86 %\n",
      "Maximum Accuracy - 36.98 %\n",
      "\n",
      "RESULTS OF 30-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 36.46 %\n",
      "Minimum Accuracy - 35.33 %\n",
      "Maximum Accuracy - 37.69 %\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "First Word Features Classifier\n",
      "RESULTS OF 5-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 22.21 %\n",
      "Minimum Accuracy - 21.81 %\n",
      "Maximum Accuracy - 22.41 %\n",
      "\n",
      "RESULTS OF 10-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 22.16 %\n",
      "Minimum Accuracy - 21.71 %\n",
      "Maximum Accuracy - 22.92 %\n",
      "\n",
      "RESULTS OF 30-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 22.16 %\n",
      "Minimum Accuracy - 21.2 %\n",
      "Maximum Accuracy - 22.94 %\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Last Word Features Classifier\n",
      "RESULTS OF 5-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 22.5 %\n",
      "Minimum Accuracy - 22.35 %\n",
      "Maximum Accuracy - 22.59 %\n",
      "\n",
      "RESULTS OF 10-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 22.5 %\n",
      "Minimum Accuracy - 22.21 %\n",
      "Maximum Accuracy - 22.75 %\n",
      "\n",
      "RESULTS OF 30-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 22.46 %\n",
      "Minimum Accuracy - 21.42 %\n",
      "Maximum Accuracy - 23.27 %\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sentence Length Features Classifier\n",
      "RESULTS OF 5-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 15.15 %\n",
      "Minimum Accuracy - 14.82 %\n",
      "Maximum Accuracy - 15.41 %\n",
      "\n",
      "RESULTS OF 10-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 15.14 %\n",
      "Minimum Accuracy - 14.68 %\n",
      "Maximum Accuracy - 15.61 %\n",
      "\n",
      "RESULTS OF 30-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 14.99 %\n",
      "Minimum Accuracy - 13.45 %\n",
      "Maximum Accuracy - 16.8 %\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Features Classifier\n",
      "RESULTS OF 5-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 25.27 %\n",
      "Minimum Accuracy - 25.2 %\n",
      "Maximum Accuracy - 25.38 %\n",
      "\n",
      "RESULTS OF 10-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 25.31 %\n",
      "Minimum Accuracy - 25.01 %\n",
      "Maximum Accuracy - 25.62 %\n",
      "\n",
      "RESULTS OF 30-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 25.31 %\n",
      "Minimum Accuracy - 24.75 %\n",
      "Maximum Accuracy - 26.19 %\n"
     ]
    }
   ],
   "source": [
    "#NLTK Naive Bayes Classifiers\n",
    "import random\n",
    "\n",
    "#Create function for cross validation, default set to five-fold\n",
    "def cross_val(features, folds = 5):\n",
    "    partition_length = int(len(features) / int(folds))\n",
    "    accuracy = []\n",
    "    for fold in list(range(1, int(folds)+1)):\n",
    "        train = features[ : partition_length*(fold-1)]\n",
    "        train = train + features[partition_length*fold : ]\n",
    "        test = features[partition_length*(fold-1) : partition_length*fold]\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "        accuracy.append(nltk.classify.accuracy(classifier, test))\n",
    "    print('RESULTS OF ' + str(int(folds)) + '-FOLD CROSS VALIDATION:')\n",
    "    print('Average Accuracy -', round((sum(accuracy) / len(accuracy))*100, 2), '%')\n",
    "    print('Minimum Accuracy -', round(min(accuracy)*100, 2), '%')\n",
    "    print('Maximum Accuracy -', round(max(accuracy)*100, 2), '%')\n",
    "\n",
    "\n",
    "#Word Features\n",
    "random.shuffle(word_features)\n",
    "print('Word Features Classifier')\n",
    "cross_val(word_features)\n",
    "print()\n",
    "cross_val(word_features,10)\n",
    "print()\n",
    "cross_val(word_features,30)\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "#First Word Features\n",
    "random.shuffle(first_word_features)\n",
    "print('First Word Features Classifier')\n",
    "cross_val(first_word_features)\n",
    "print()\n",
    "cross_val(first_word_features,10)\n",
    "print()\n",
    "cross_val(first_word_features,30)\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "#Last Word Features\n",
    "random.shuffle(last_word_features)\n",
    "print('Last Word Features Classifier')\n",
    "cross_val(last_word_features)\n",
    "print()\n",
    "cross_val(last_word_features,10)\n",
    "print()\n",
    "cross_val(last_word_features,30)\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "#Sentence Length Features\n",
    "random.shuffle(sent_length_features)\n",
    "print('Sentence Length Features Classifier')\n",
    "cross_val(sent_length_features)\n",
    "print()\n",
    "cross_val(sent_length_features,10)\n",
    "print()\n",
    "cross_val(sent_length_features,30)\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "#ALL Features\n",
    "all_features = word_features + first_word_features + last_word_features + sent_length_features\n",
    "random.shuffle(all_features)\n",
    "print('All Features Classifier')\n",
    "cross_val(all_features)\n",
    "print()\n",
    "cross_val(all_features,10)\n",
    "print()\n",
    "cross_val(all_features,30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that none of the feature sets have noteworthy accuracy. Perhaps we should be using using the feature sets to predict between two categories instead of all of the NLTK Brown corpus categories. We will reduce down to only the \"news\" and \"humor\" categories, then try this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Features Classifier\n",
      "RESULTS OF 5-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 66.9 %\n",
      "Minimum Accuracy - 64.85 %\n",
      "Maximum Accuracy - 68.48 %\n",
      "\n",
      "RESULTS OF 10-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 67.2 %\n",
      "Minimum Accuracy - 61.64 %\n",
      "Maximum Accuracy - 70.18 %\n",
      "\n",
      "RESULTS OF 30-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 67.5 %\n",
      "Minimum Accuracy - 59.02 %\n",
      "Maximum Accuracy - 79.23 %\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "First Word Features Classifier\n",
      "RESULTS OF 5-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 83.24 %\n",
      "Minimum Accuracy - 82.32 %\n",
      "Maximum Accuracy - 84.29 %\n",
      "\n",
      "RESULTS OF 10-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 83.22 %\n",
      "Minimum Accuracy - 82.11 %\n",
      "Maximum Accuracy - 84.68 %\n",
      "\n",
      "RESULTS OF 30-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 83.15 %\n",
      "Minimum Accuracy - 79.53 %\n",
      "Maximum Accuracy - 85.71 %\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Last Word Features Classifier\n",
      "RESULTS OF 5-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 83.99 %\n",
      "Minimum Accuracy - 83.75 %\n",
      "Maximum Accuracy - 84.22 %\n",
      "\n",
      "RESULTS OF 10-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 84.11 %\n",
      "Minimum Accuracy - 83.25 %\n",
      "Maximum Accuracy - 84.91 %\n",
      "\n",
      "RESULTS OF 30-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 84.12 %\n",
      "Minimum Accuracy - 81.34 %\n",
      "Maximum Accuracy - 86.32 %\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sentence Length Features Classifier\n",
      "RESULTS OF 5-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 81.37 %\n",
      "Minimum Accuracy - 79.91 %\n",
      "Maximum Accuracy - 83.17 %\n",
      "\n",
      "RESULTS OF 10-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 81.38 %\n",
      "Minimum Accuracy - 78.48 %\n",
      "Maximum Accuracy - 83.42 %\n",
      "\n",
      "RESULTS OF 30-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 81.38 %\n",
      "Minimum Accuracy - 75.13 %\n",
      "Maximum Accuracy - 86.24 %\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Features Classifier\n",
      "RESULTS OF 5-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 81.6 %\n",
      "Minimum Accuracy - 81.13 %\n",
      "Maximum Accuracy - 81.81 %\n",
      "\n",
      "RESULTS OF 10-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 81.61 %\n",
      "Minimum Accuracy - 80.65 %\n",
      "Maximum Accuracy - 82.39 %\n",
      "\n",
      "RESULTS OF 30-FOLD CROSS VALIDATION:\n",
      "Average Accuracy - 81.66 %\n",
      "Minimum Accuracy - 79.32 %\n",
      "Maximum Accuracy - 83.7 %\n"
     ]
    }
   ],
   "source": [
    "#Filter feature sets to only news and humor categories\n",
    "word_features = [(featdict,category) for (featdict,category) in word_features if category in ('news', 'humor')]\n",
    "first_word_features = [(featdict,category) for (featdict,category) in first_word_features if category in ('news', 'humor')]\n",
    "last_word_features = [(featdict,category) for (featdict,category) in last_word_features if category in ('news', 'humor')]\n",
    "sent_length_features = [(featdict,category) for (featdict,category) in sent_length_features if category in ('news', 'humor')]\n",
    "\n",
    "\n",
    "#Word Features\n",
    "random.shuffle(word_features)\n",
    "print('Word Features Classifier')\n",
    "cross_val(word_features)\n",
    "print()\n",
    "cross_val(word_features,10)\n",
    "print()\n",
    "cross_val(word_features,30)\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "#First Word Features\n",
    "random.shuffle(first_word_features)\n",
    "print('First Word Features Classifier')\n",
    "cross_val(first_word_features)\n",
    "print()\n",
    "cross_val(first_word_features,10)\n",
    "print()\n",
    "cross_val(first_word_features,30)\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "#Last Word Features\n",
    "random.shuffle(last_word_features)\n",
    "print('Last Word Features Classifier')\n",
    "cross_val(last_word_features)\n",
    "print()\n",
    "cross_val(last_word_features,10)\n",
    "print()\n",
    "cross_val(last_word_features,30)\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "#Sentence Length Features\n",
    "random.shuffle(sent_length_features)\n",
    "print('Sentence Length Features Classifier')\n",
    "cross_val(sent_length_features)\n",
    "print()\n",
    "cross_val(sent_length_features,10)\n",
    "print()\n",
    "cross_val(sent_length_features,30)\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "#ALL Features\n",
    "all_features = word_features + first_word_features + last_word_features + sent_length_features\n",
    "random.shuffle(all_features)\n",
    "print('All Features Classifier')\n",
    "cross_val(all_features)\n",
    "print()\n",
    "cross_val(all_features,10)\n",
    "print()\n",
    "cross_val(all_features,30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Let's take a look at the most informative features for each of the feature sets! We will stick to just a ten-fold cross validation for the sake of simplicity, and will show the top 5 most informative features for each of the 10 classifiers generated for each feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Most Informative Features\n",
      "ROUND 1\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     12.2 : 1.0\n",
      "                    Word = 'heart'         humor : news   =      9.9 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =      8.7 : 1.0\n",
      "                    Word = 'american'       news : humor  =      8.2 : 1.0\n",
      "                    Word = 'mind'          humor : news   =      7.5 : 1.0\n",
      "\n",
      "ROUND 2\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     13.7 : 1.0\n",
      "                    Word = 'american'       news : humor  =      9.5 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =      8.9 : 1.0\n",
      "                    Word = 'heart'         humor : news   =      8.9 : 1.0\n",
      "                    Word = 'english'       humor : news   =      7.7 : 1.0\n",
      "\n",
      "ROUND 3\n",
      "Most Informative Features\n",
      "                    Word = 'eyes'          humor : news   =     10.0 : 1.0\n",
      "                    Word = 'heart'         humor : news   =     10.0 : 1.0\n",
      "                    Word = 'looked'        humor : news   =     10.0 : 1.0\n",
      "                    Word = 'american'       news : humor  =      9.3 : 1.0\n",
      "                    Word = 'english'       humor : news   =      8.8 : 1.0\n",
      "\n",
      "ROUND 4\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     13.3 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =      9.8 : 1.0\n",
      "                    Word = 'heart'         humor : news   =      9.8 : 1.0\n",
      "                    Word = 'american'       news : humor  =      9.4 : 1.0\n",
      "                    Word = 'mind'          humor : news   =      8.7 : 1.0\n",
      "\n",
      "ROUND 5\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     12.3 : 1.0\n",
      "                    Word = 'us'            humor : news   =     10.2 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =     10.0 : 1.0\n",
      "                    Word = 'mind'          humor : news   =      8.8 : 1.0\n",
      "                    Word = 'american'       news : humor  =      7.8 : 1.0\n",
      "\n",
      "ROUND 6\n",
      "Most Informative Features\n",
      "                    Word = 'heart'         humor : news   =     10.2 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =      9.0 : 1.0\n",
      "                    Word = 'american'       news : humor  =      8.3 : 1.0\n",
      "                    Word = 'mind'          humor : news   =      7.8 : 1.0\n",
      "                    Word = 'work'           news : humor  =      7.2 : 1.0\n",
      "\n",
      "ROUND 7\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     12.3 : 1.0\n",
      "                    Word = 'american'       news : humor  =      8.5 : 1.0\n",
      "                    Word = 'mind'          humor : news   =      7.6 : 1.0\n",
      "                    Word = 'work'           news : humor  =      7.0 : 1.0\n",
      "                    Word = 'else'          humor : news   =      6.4 : 1.0\n",
      "\n",
      "ROUND 8\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     12.4 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =     10.1 : 1.0\n",
      "                    Word = 'mind'          humor : news   =      8.9 : 1.0\n",
      "                    Word = 'american'       news : humor  =      8.8 : 1.0\n",
      "                    Word = 'states'         news : humor  =      6.6 : 1.0\n",
      "\n",
      "ROUND 9\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     13.5 : 1.0\n",
      "                    Word = 'heart'         humor : news   =      8.8 : 1.0\n",
      "                    Word = 'american'       news : humor  =      8.5 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =      7.6 : 1.0\n",
      "                    Word = 'least'         humor : news   =      7.6 : 1.0\n",
      "\n",
      "ROUND 10\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     13.5 : 1.0\n",
      "                    Word = 'heart'         humor : news   =     10.0 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =      8.8 : 1.0\n",
      "                    Word = 'anything'      humor : news   =      6.5 : 1.0\n",
      "                    Word = 'light'         humor : news   =      6.5 : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Word Features most informative features\n",
    "print('Word Most Informative Features')\n",
    "partition_length = int(len(word_features) / 10)\n",
    "for fold in list(range(1, 11)):\n",
    "    train = word_features[ : partition_length*(fold-1)]\n",
    "    train = train + word_features[partition_length*fold : ]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "    print('ROUND', fold)\n",
    "    classifier.show_most_informative_features(5)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Word Most Informative Features\n",
      "ROUND 1\n",
      "Most Informative Features\n",
      "              First Word = 'mouth'         humor : news   =     22.0 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     22.0 : 1.0\n",
      "              First Word = 'president'      news : humor  =     18.8 : 1.0\n",
      "              First Word = 'sat'           humor : news   =     18.5 : 1.0\n",
      "              First Word = 'looked'        humor : news   =     17.0 : 1.0\n",
      "\n",
      "ROUND 2\n",
      "Most Informative Features\n",
      "              First Word = 'president'      news : humor  =     20.5 : 1.0\n",
      "              First Word = 'sat'           humor : news   =     18.3 : 1.0\n",
      "              First Word = 'looked'        humor : news   =     16.8 : 1.0\n",
      "              First Word = 'blanche'       humor : news   =     16.0 : 1.0\n",
      "              First Word = 'horse'         humor : news   =     14.8 : 1.0\n",
      "\n",
      "ROUND 3\n",
      "Most Informative Features\n",
      "              First Word = 'horse'         humor : news   =     24.8 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     21.9 : 1.0\n",
      "              First Word = 'voice'         humor : news   =     21.9 : 1.0\n",
      "              First Word = 'president'      news : humor  =     19.9 : 1.0\n",
      "              First Word = 'sat'           humor : news   =     14.9 : 1.0\n",
      "\n",
      "ROUND 4\n",
      "Most Informative Features\n",
      "              First Word = 'nice'          humor : news   =     24.9 : 1.0\n",
      "              First Word = 'president'      news : humor  =     19.2 : 1.0\n",
      "              First Word = 'sat'           humor : news   =     18.4 : 1.0\n",
      "              First Word = 'horse'         humor : news   =     16.7 : 1.0\n",
      "              First Word = 'moment'        humor : news   =     16.7 : 1.0\n",
      "\n",
      "ROUND 5\n",
      "Most Informative Features\n",
      "              First Word = 'nice'          humor : news   =     25.0 : 1.0\n",
      "              First Word = 'horse'         humor : news   =     22.0 : 1.0\n",
      "              First Word = 'president'      news : humor  =     19.1 : 1.0\n",
      "              First Word = 'sat'           humor : news   =     16.7 : 1.0\n",
      "              First Word = 'yes'           humor : news   =     16.1 : 1.0\n",
      "\n",
      "ROUND 6\n",
      "Most Informative Features\n",
      "              First Word = 'sat'           humor : news   =     30.8 : 1.0\n",
      "              First Word = 'president'      news : humor  =     19.5 : 1.0\n",
      "              First Word = 'blanche'       humor : news   =     19.0 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     19.0 : 1.0\n",
      "              First Word = 'horse'         humor : news   =     14.9 : 1.0\n",
      "\n",
      "ROUND 7\n",
      "Most Informative Features\n",
      "              First Word = 'president'      news : humor  =     19.4 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     19.0 : 1.0\n",
      "              First Word = 'mouth'         humor : news   =     16.1 : 1.0\n",
      "              First Word = 'horse'         humor : news   =     14.9 : 1.0\n",
      "              First Word = 'sat'           humor : news   =     14.9 : 1.0\n",
      "\n",
      "ROUND 8\n",
      "Most Informative Features\n",
      "              First Word = 'nice'          humor : news   =     21.8 : 1.0\n",
      "              First Word = 'president'      news : humor  =     19.2 : 1.0\n",
      "              First Word = 'looked'        humor : news   =     16.8 : 1.0\n",
      "              First Word = 'sat'           humor : news   =     16.6 : 1.0\n",
      "              First Word = 'horse'         humor : news   =     14.8 : 1.0\n",
      "\n",
      "ROUND 9\n",
      "Most Informative Features\n",
      "              First Word = 'nice'          humor : news   =     24.7 : 1.0\n",
      "              First Word = 'voice'         humor : news   =     18.9 : 1.0\n",
      "              First Word = 'horse'         humor : news   =     16.5 : 1.0\n",
      "              First Word = 'everyone'      humor : news   =     13.1 : 1.0\n",
      "              First Word = 'mouth'         humor : news   =     13.1 : 1.0\n",
      "\n",
      "ROUND 10\n",
      "Most Informative Features\n",
      "              First Word = 'sat'           humor : news   =     27.5 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     24.6 : 1.0\n",
      "              First Word = 'president'      news : humor  =     19.1 : 1.0\n",
      "              First Word = 'looked'        humor : news   =     16.7 : 1.0\n",
      "              First Word = 'friend'        humor : news   =     14.7 : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#First Word Features length most informative features\n",
    "print('First Word Most Informative Features')\n",
    "partition_length = int(len(first_word_features) / 10)\n",
    "for fold in list(range(1, 11)):\n",
    "    train = first_word_features[ : partition_length*(fold-1)]\n",
    "    train = train + first_word_features[partition_length*fold : ]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "    print('ROUND', fold)\n",
    "    classifier.show_most_informative_features(5)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Word Most Informative Features\n",
      "ROUND 1\n",
      "Most Informative Features\n",
      "               Last Word = 'horse'         humor : news   =     22.9 : 1.0\n",
      "               Last Word = 'shoes'         humor : news   =     19.9 : 1.0\n",
      "               Last Word = 'sat'           humor : news   =     19.3 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     18.5 : 1.0\n",
      "               Last Word = 'glass'         humor : news   =     16.8 : 1.0\n",
      "\n",
      "ROUND 2\n",
      "Most Informative Features\n",
      "               Last Word = 'horse'         humor : news   =     22.9 : 1.0\n",
      "               Last Word = 'shoes'         humor : news   =     19.8 : 1.0\n",
      "               Last Word = 'sat'           humor : news   =     19.2 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     18.6 : 1.0\n",
      "               Last Word = 'glass'         humor : news   =     16.8 : 1.0\n",
      "\n",
      "ROUND 3\n",
      "Most Informative Features\n",
      "               Last Word = 'sat'           humor : news   =     19.3 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     19.0 : 1.0\n",
      "               Last Word = 'horse'         humor : news   =     17.4 : 1.0\n",
      "               Last Word = 'love'          humor : news   =     16.8 : 1.0\n",
      "               Last Word = 'shoes'         humor : news   =     16.8 : 1.0\n",
      "\n",
      "ROUND 4\n",
      "Most Informative Features\n",
      "               Last Word = 'sat'           humor : news   =     19.4 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     18.0 : 1.0\n",
      "               Last Word = 'heart'         humor : news   =     17.8 : 1.0\n",
      "               Last Word = 'teeth'         humor : news   =     16.9 : 1.0\n",
      "               Last Word = 'horse'         humor : news   =     15.7 : 1.0\n",
      "\n",
      "ROUND 5\n",
      "Most Informative Features\n",
      "               Last Word = 'mouth'         humor : news   =     22.8 : 1.0\n",
      "               Last Word = 'shoes'         humor : news   =     19.8 : 1.0\n",
      "               Last Word = 'voice'         humor : news   =     19.8 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     18.2 : 1.0\n",
      "               Last Word = 'looked'        humor : news   =     17.6 : 1.0\n",
      "\n",
      "ROUND 6\n",
      "Most Informative Features\n",
      "               Last Word = 'sat'           humor : news   =     26.1 : 1.0\n",
      "               Last Word = 'shoes'         humor : news   =     19.9 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     18.6 : 1.0\n",
      "               Last Word = 'horse'         humor : news   =     17.5 : 1.0\n",
      "               Last Word = 'hair'          humor : news   =     16.9 : 1.0\n",
      "\n",
      "ROUND 7\n",
      "Most Informative Features\n",
      "               Last Word = 'heart'         humor : news   =     17.8 : 1.0\n",
      "               Last Word = 'horse'         humor : news   =     17.5 : 1.0\n",
      "               Last Word = 'described'     humor : news   =     16.9 : 1.0\n",
      "               Last Word = 'hair'          humor : news   =     16.9 : 1.0\n",
      "               Last Word = 'teeth'         humor : news   =     16.9 : 1.0\n",
      "\n",
      "ROUND 8\n",
      "Most Informative Features\n",
      "               Last Word = 'voice'         humor : news   =     22.8 : 1.0\n",
      "               Last Word = 'shoes'         humor : news   =     19.8 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     19.2 : 1.0\n",
      "               Last Word = 'horse'         humor : news   =     17.3 : 1.0\n",
      "               Last Word = 'sat'           humor : news   =     17.3 : 1.0\n",
      "\n",
      "ROUND 9\n",
      "Most Informative Features\n",
      "               Last Word = 'shoes'         humor : news   =     19.9 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     18.3 : 1.0\n",
      "               Last Word = 'sat'           humor : news   =     17.4 : 1.0\n",
      "               Last Word = 'glass'         humor : news   =     16.8 : 1.0\n",
      "               Last Word = 'teeth'         humor : news   =     16.8 : 1.0\n",
      "\n",
      "ROUND 10\n",
      "Most Informative Features\n",
      "               Last Word = 'sat'           humor : news   =     23.0 : 1.0\n",
      "               Last Word = 'mouth'         humor : news   =     19.9 : 1.0\n",
      "               Last Word = 'moment'        humor : news   =     19.3 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     19.1 : 1.0\n",
      "               Last Word = 'horse'         humor : news   =     17.5 : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Last Word Features most informative features\n",
    "print('Last Word Most Informative Features')\n",
    "partition_length = int(len(last_word_features) / 10)\n",
    "for fold in list(range(1, 11)):\n",
    "    train = last_word_features[ : partition_length*(fold-1)]\n",
    "    train = train + last_word_features[partition_length*fold : ]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "    print('ROUND', fold)\n",
    "    classifier.show_most_informative_features(5)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Length Most Informative Features\n",
      "ROUND 1\n",
      "Most Informative Features\n",
      "         Sentence Length = 54              humor : news   =      7.1 : 1.0\n",
      "         Sentence Length = 57              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 66              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 53              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 44              humor : news   =      2.5 : 1.0\n",
      "\n",
      "ROUND 2\n",
      "Most Informative Features\n",
      "         Sentence Length = 53              humor : news   =      7.0 : 1.0\n",
      "         Sentence Length = 54              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 59              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 60              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 66              humor : news   =      4.2 : 1.0\n",
      "\n",
      "ROUND 3\n",
      "Most Informative Features\n",
      "         Sentence Length = 56              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 59              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 66              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 53              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 45              humor : news   =      3.5 : 1.0\n",
      "\n",
      "ROUND 4\n",
      "Most Informative Features\n",
      "         Sentence Length = 54              humor : news   =      4.4 : 1.0\n",
      "         Sentence Length = 59              humor : news   =      4.4 : 1.0\n",
      "         Sentence Length = 66              humor : news   =      4.4 : 1.0\n",
      "         Sentence Length = 47              humor : news   =      2.6 : 1.0\n",
      "         Sentence Length = 53              humor : news   =      2.6 : 1.0\n",
      "\n",
      "ROUND 5\n",
      "Most Informative Features\n",
      "         Sentence Length = 53              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 54              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 59              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 56              humor : news   =      2.6 : 1.0\n",
      "         Sentence Length = 57              humor : news   =      2.6 : 1.0\n",
      "\n",
      "ROUND 6\n",
      "Most Informative Features\n",
      "         Sentence Length = 53              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 54              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 57              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 66              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 30               news : humor  =      3.3 : 1.0\n",
      "\n",
      "ROUND 7\n",
      "Most Informative Features\n",
      "         Sentence Length = 53              humor : news   =      7.0 : 1.0\n",
      "         Sentence Length = 54              humor : news   =      7.0 : 1.0\n",
      "         Sentence Length = 59              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 66              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 56              humor : news   =      2.5 : 1.0\n",
      "\n",
      "ROUND 8\n",
      "Most Informative Features\n",
      "         Sentence Length = 53              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 59              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 61              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 66              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 51              humor : news   =      3.0 : 1.0\n",
      "\n",
      "ROUND 9\n",
      "Most Informative Features\n",
      "         Sentence Length = 54              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 59              humor : news   =      4.2 : 1.0\n",
      "         Sentence Length = 45              humor : news   =      2.5 : 1.0\n",
      "         Sentence Length = 47              humor : news   =      2.5 : 1.0\n",
      "         Sentence Length = 53              humor : news   =      2.5 : 1.0\n",
      "\n",
      "ROUND 10\n",
      "Most Informative Features\n",
      "         Sentence Length = 56              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 59              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 60              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 66              humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = 53              humor : news   =      4.3 : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sentence length most informative features\n",
    "print('Sentence Length Most Informative Features')\n",
    "partition_length = int(len(sent_length_features) / 10)\n",
    "for fold in list(range(1, 11)):\n",
    "    train = sent_length_features[ : partition_length*(fold-1)]\n",
    "    train = train + sent_length_features[partition_length*fold : ]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "    print('ROUND', fold)\n",
    "    classifier.show_most_informative_features(5)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Most Informative Features\n",
      "ROUND 1\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     26.8 : 1.0\n",
      "              First Word = 'horse'         humor : news   =     26.4 : 1.0\n",
      "              First Word = 'president'      news : humor  =     21.2 : 1.0\n",
      "                    Word = 'seemed'        humor : news   =     21.2 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =     21.2 : 1.0\n",
      "\n",
      "ROUND 2\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     32.7 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =     24.2 : 1.0\n",
      "                    Word = 'heart'         humor : news   =     21.3 : 1.0\n",
      "              First Word = 'president'      news : humor  =     20.5 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     20.4 : 1.0\n",
      "\n",
      "ROUND 3\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     26.7 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     21.6 : 1.0\n",
      "                    Word = 'english'       humor : news   =     21.1 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =     21.1 : 1.0\n",
      "                    Word = 'heart'         humor : news   =     21.1 : 1.0\n",
      "\n",
      "ROUND 4\n",
      "Most Informative Features\n",
      "              First Word = 'sat'           humor : news   =     26.5 : 1.0\n",
      "                    Word = 'looked'        humor : news   =     24.0 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =     21.2 : 1.0\n",
      "                    Word = 'mind'          humor : news   =     21.2 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     20.9 : 1.0\n",
      "\n",
      "ROUND 5\n",
      "Most Informative Features\n",
      "                    Word = 'eyes'          humor : news   =     24.0 : 1.0\n",
      "                    Word = 'heart'         humor : news   =     24.0 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     23.7 : 1.0\n",
      "              First Word = 'blanche'       humor : news   =     20.9 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     20.9 : 1.0\n",
      "\n",
      "ROUND 6\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     29.8 : 1.0\n",
      "                    Word = 'heart'         humor : news   =     21.3 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     21.0 : 1.0\n",
      "               Last Word = 'mouth'         humor : news   =     21.0 : 1.0\n",
      "              First Word = 'president'      news : humor  =     20.2 : 1.0\n",
      "\n",
      "ROUND 7\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     29.5 : 1.0\n",
      "                    Word = 'heart'         humor : news   =     23.9 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =     21.1 : 1.0\n",
      "                    Word = 'mind'          humor : news   =     21.1 : 1.0\n",
      "              First Word = 'blanche'       humor : news   =     20.8 : 1.0\n",
      "\n",
      "ROUND 8\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     30.0 : 1.0\n",
      "              First Word = 'sat'           humor : news   =     29.6 : 1.0\n",
      "              First Word = 'horse'         humor : news   =     23.9 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     23.9 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =     21.4 : 1.0\n",
      "\n",
      "ROUND 9\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     32.7 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     23.8 : 1.0\n",
      "               Last Word = 'sat'           humor : news   =     23.8 : 1.0\n",
      "                    Word = 'heart'         humor : news   =     21.3 : 1.0\n",
      "               Last Word = 'president'      news : humor  =     20.6 : 1.0\n",
      "\n",
      "ROUND 10\n",
      "Most Informative Features\n",
      "                    Word = 'looked'        humor : news   =     32.5 : 1.0\n",
      "               Last Word = 'sat'           humor : news   =     26.5 : 1.0\n",
      "                    Word = 'eyes'          humor : news   =     24.1 : 1.0\n",
      "                    Word = 'heart'         humor : news   =     24.1 : 1.0\n",
      "              First Word = 'nice'          humor : news   =     23.7 : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ALL Features most informative features\n",
    "print('All Most Informative Features')\n",
    "partition_length = int(len(all_features) / 10)\n",
    "for fold in list(range(1, 11)):\n",
    "    train = all_features[ : partition_length*(fold-1)]\n",
    "    train = train + all_features[partition_length*fold : ]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "    print('ROUND', fold)\n",
    "    classifier.show_most_informative_features(5)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Sentence Length Most Informatve Features are technically the \"most informative features\", they are hardly informative. These would be much more intuituve if we used sentence length ranges instead of discrete sentence lengths. Let's update the Sentence Length Features using sentence length ranges, then look at the first 10 and last 10 features and the top 10 most informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE LENGTH ANALYSIS:\n",
      "Minimum Length: 0\n",
      "Minimum Length: 91\n",
      "Average Length: 18.043164200140943\n",
      "\n",
      "\n",
      "First 10 and Last 10 Updated Sentence Length Features\n",
      "({'Sentence Length': '16-20 Words'}, 'news')\n",
      "({'Sentence Length': '21-30 Words'}, 'news')\n",
      "({'Sentence Length': '16-20 Words'}, 'news')\n",
      "({'Sentence Length': '0-5 Words'}, 'news')\n",
      "({'Sentence Length': '21-30 Words'}, 'news')\n",
      "({'Sentence Length': '0-5 Words'}, 'news')\n",
      "({'Sentence Length': '31-40 Words'}, 'humor')\n",
      "({'Sentence Length': '31-40 Words'}, 'news')\n",
      "({'Sentence Length': '6-10 Words'}, 'humor')\n",
      "({'Sentence Length': '21-30 Words'}, 'humor')\n"
     ]
    }
   ],
   "source": [
    "#Check lengths of sentences\n",
    "sent_lengths = [feature[0]['Sentence Length'] for feature in sent_length_features]\n",
    "print('SENTENCE LENGTH ANALYSIS:')\n",
    "print('Minimum Length:', min(sent_lengths))\n",
    "print('Maximum Length:', max(sent_lengths))\n",
    "print('Average Length:', sum(sent_lengths) / len(sent_lengths))\n",
    "\n",
    "#Update sentence length feature set\n",
    "for (featdict, category) in sent_length_features:\n",
    "    if featdict['Sentence Length'] >= 0 and featdict['Sentence Length'] <= 5: featdict.update({'Sentence Length' : '0-5 Words'})\n",
    "    elif featdict['Sentence Length'] >= 6 and featdict['Sentence Length'] <= 10: featdict.update({'Sentence Length' : '6-10 Words'})\n",
    "    elif featdict['Sentence Length'] >= 10 and featdict['Sentence Length'] <= 15: featdict.update({'Sentence Length' : '11-15 Words'})\n",
    "    elif featdict['Sentence Length'] >= 16 and featdict['Sentence Length'] <= 20: featdict.update({'Sentence Length' : '16-20 Words'})\n",
    "    elif featdict['Sentence Length'] >= 21 and featdict['Sentence Length'] <= 30: featdict.update({'Sentence Length' : '21-30 Words'})\n",
    "    elif featdict['Sentence Length'] >= 31 and featdict['Sentence Length'] <= 40: featdict.update({'Sentence Length' : '31-40 Words'})\n",
    "    elif featdict['Sentence Length'] >= 41 and featdict['Sentence Length'] <= 50: featdict.update({'Sentence Length' : '41-50 Words'})\n",
    "    elif featdict['Sentence Length'] >= 51 and featdict['Sentence Length'] <= 75: featdict.update({'Sentence Length' : '51-75 Words'})\n",
    "    elif featdict['Sentence Length'] >= 76 and featdict['Sentence Length'] <= 100: featdict.update({'Sentence Length' : '76-100 Words'})\n",
    "    elif featdict['Sentence Length'] >= 101: featdict.update({'Sentence Length' : 'More Than 100 Words'})\n",
    "\n",
    "#Look at first 10 and last 10 updated features\n",
    "print('\\n\\nFirst 10 and Last 10 Updated Sentence Length Features')\n",
    "for i in list(range(5)) + list(range(-5,0)): print(sent_length_features[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Length Most Informative Features\n",
      "ROUND 1\n",
      "Most Informative Features\n",
      "         Sentence Length = '76-100 Words'  humor : news   =      4.4 : 1.0\n",
      "         Sentence Length = '51-75 Words'   humor : news   =      2.4 : 1.0\n",
      "         Sentence Length = '6-10 Words'    humor : news   =      1.6 : 1.0\n",
      "         Sentence Length = '21-30 Words'    news : humor  =      1.3 : 1.0\n",
      "         Sentence Length = '0-5 Words'     humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '11-15 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '16-20 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '41-50 Words'   humor : news   =      1.1 : 1.0\n",
      "         Sentence Length = '31-40 Words'    news : humor  =      1.0 : 1.0\n",
      "\n",
      "ROUND 2\n",
      "Most Informative Features\n",
      "         Sentence Length = '76-100 Words'  humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = '51-75 Words'   humor : news   =      2.3 : 1.0\n",
      "         Sentence Length = '6-10 Words'    humor : news   =      1.6 : 1.0\n",
      "         Sentence Length = '21-30 Words'    news : humor  =      1.4 : 1.0\n",
      "         Sentence Length = '0-5 Words'     humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '16-20 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '11-15 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '41-50 Words'   humor : news   =      1.0 : 1.0\n",
      "         Sentence Length = '31-40 Words'   humor : news   =      1.0 : 1.0\n",
      "\n",
      "ROUND 3\n",
      "Most Informative Features\n",
      "         Sentence Length = '76-100 Words'  humor : news   =      4.4 : 1.0\n",
      "         Sentence Length = '51-75 Words'   humor : news   =      2.1 : 1.0\n",
      "         Sentence Length = '6-10 Words'    humor : news   =      1.7 : 1.0\n",
      "         Sentence Length = '21-30 Words'    news : humor  =      1.4 : 1.0\n",
      "         Sentence Length = '16-20 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '11-15 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '0-5 Words'     humor : news   =      1.1 : 1.0\n",
      "         Sentence Length = '41-50 Words'   humor : news   =      1.1 : 1.0\n",
      "         Sentence Length = '31-40 Words'   humor : news   =      1.0 : 1.0\n",
      "\n",
      "ROUND 4\n",
      "Most Informative Features\n",
      "         Sentence Length = '76-100 Words'  humor : news   =      4.5 : 1.0\n",
      "         Sentence Length = '51-75 Words'   humor : news   =      2.0 : 1.0\n",
      "         Sentence Length = '6-10 Words'    humor : news   =      1.6 : 1.0\n",
      "         Sentence Length = '41-50 Words'   humor : news   =      1.3 : 1.0\n",
      "         Sentence Length = '21-30 Words'    news : humor  =      1.3 : 1.0\n",
      "         Sentence Length = '16-20 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '11-15 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '0-5 Words'     humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '31-40 Words'    news : humor  =      1.1 : 1.0\n",
      "\n",
      "ROUND 5\n",
      "Most Informative Features\n",
      "         Sentence Length = '76-100 Words'  humor : news   =      7.4 : 1.0\n",
      "         Sentence Length = '51-75 Words'   humor : news   =      2.1 : 1.0\n",
      "         Sentence Length = '6-10 Words'    humor : news   =      1.6 : 1.0\n",
      "         Sentence Length = '21-30 Words'    news : humor  =      1.4 : 1.0\n",
      "         Sentence Length = '0-5 Words'     humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '16-20 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '11-15 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '41-50 Words'   humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '31-40 Words'   humor : news   =      1.0 : 1.0\n",
      "\n",
      "ROUND 6\n",
      "Most Informative Features\n",
      "         Sentence Length = '76-100 Words'  humor : news   =      4.4 : 1.0\n",
      "         Sentence Length = '51-75 Words'   humor : news   =      2.0 : 1.0\n",
      "         Sentence Length = '6-10 Words'    humor : news   =      1.6 : 1.0\n",
      "         Sentence Length = '21-30 Words'    news : humor  =      1.4 : 1.0\n",
      "         Sentence Length = '41-50 Words'   humor : news   =      1.3 : 1.0\n",
      "         Sentence Length = '0-5 Words'     humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '16-20 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '11-15 Words'    news : humor  =      1.1 : 1.0\n",
      "         Sentence Length = '31-40 Words'    news : humor  =      1.1 : 1.0\n",
      "\n",
      "ROUND 7\n",
      "Most Informative Features\n",
      "         Sentence Length = '76-100 Words'  humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = '51-75 Words'   humor : news   =      2.5 : 1.0\n",
      "         Sentence Length = '6-10 Words'    humor : news   =      1.6 : 1.0\n",
      "         Sentence Length = '21-30 Words'    news : humor  =      1.4 : 1.0\n",
      "         Sentence Length = '16-20 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '0-5 Words'     humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '11-15 Words'    news : humor  =      1.1 : 1.0\n",
      "         Sentence Length = '41-50 Words'   humor : news   =      1.1 : 1.0\n",
      "         Sentence Length = '31-40 Words'    news : humor  =      1.0 : 1.0\n",
      "\n",
      "ROUND 8\n",
      "Most Informative Features\n",
      "         Sentence Length = '76-100 Words'  humor : news   =      4.3 : 1.0\n",
      "         Sentence Length = '51-75 Words'   humor : news   =      2.4 : 1.0\n",
      "         Sentence Length = '6-10 Words'    humor : news   =      1.7 : 1.0\n",
      "         Sentence Length = '21-30 Words'    news : humor  =      1.4 : 1.0\n",
      "         Sentence Length = '11-15 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '0-5 Words'     humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '16-20 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '41-50 Words'   humor : news   =      1.1 : 1.0\n",
      "         Sentence Length = '31-40 Words'   humor : news   =      1.0 : 1.0\n",
      "\n",
      "ROUND 9\n",
      "Most Informative Features\n",
      "         Sentence Length = '76-100 Words'  humor : news   =      2.6 : 1.0\n",
      "         Sentence Length = '51-75 Words'   humor : news   =      2.0 : 1.0\n",
      "         Sentence Length = '6-10 Words'    humor : news   =      1.6 : 1.0\n",
      "         Sentence Length = '21-30 Words'    news : humor  =      1.4 : 1.0\n",
      "         Sentence Length = '41-50 Words'   humor : news   =      1.3 : 1.0\n",
      "         Sentence Length = '0-5 Words'     humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '16-20 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '11-15 Words'    news : humor  =      1.1 : 1.0\n",
      "         Sentence Length = '31-40 Words'   humor : news   =      1.0 : 1.0\n",
      "\n",
      "ROUND 10\n",
      "Most Informative Features\n",
      "         Sentence Length = '76-100 Words'  humor : news   =      4.4 : 1.0\n",
      "         Sentence Length = '51-75 Words'   humor : news   =      2.6 : 1.0\n",
      "         Sentence Length = '6-10 Words'    humor : news   =      1.6 : 1.0\n",
      "         Sentence Length = '21-30 Words'    news : humor  =      1.4 : 1.0\n",
      "         Sentence Length = '16-20 Words'    news : humor  =      1.2 : 1.0\n",
      "         Sentence Length = '41-50 Words'   humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '0-5 Words'     humor : news   =      1.2 : 1.0\n",
      "         Sentence Length = '11-15 Words'    news : humor  =      1.1 : 1.0\n",
      "         Sentence Length = '31-40 Words'    news : humor  =      1.0 : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sentence length most informative features\n",
    "print('Sentence Length Most Informative Features')\n",
    "partition_length = int(len(sent_length_features) / 10)\n",
    "for fold in list(range(1, 11)):\n",
    "    train = sent_length_features[ : partition_length*(fold-1)]\n",
    "    train = train + sent_length_features[partition_length*fold : ]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "    print('ROUND', fold)\n",
    "    classifier.show_most_informative_features(10)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at which sentence lengths correspond to which categories! Let's see how many sentences fall into each length range and category, then put the results into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>News</th>\n",
       "      <th>Humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-5 Words</th>\n",
       "      <td>694</td>\n",
       "      <td>546</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-10 Words</th>\n",
       "      <td>854</td>\n",
       "      <td>624</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16-20 Words</th>\n",
       "      <td>1030</td>\n",
       "      <td>864</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21-30 Words</th>\n",
       "      <td>1330</td>\n",
       "      <td>1139</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-40 Words</th>\n",
       "      <td>565</td>\n",
       "      <td>461</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41-50 Words</th>\n",
       "      <td>148</td>\n",
       "      <td>117</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51-75 Words</th>\n",
       "      <td>42</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76-100 Words</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentences  News  Humor\n",
       "0-5 Words           694   546    148\n",
       "6-10 Words          854   624    230\n",
       "16-20 Words        1030   864    166\n",
       "21-30 Words        1330  1139    191\n",
       "31-40 Words         565   461    104\n",
       "41-50 Words         148   117     31\n",
       "51-75 Words          42    28     14\n",
       "76-100 Words          4     2      2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create data frame\n",
    "length_ranges_DF = pandas.DataFrame(columns = ['Sorter', 'Feature', 'Sentence Length', 'Category', 'Sentences', 'News', 'Humor'])\n",
    "length_ranges_DF['Feature'] = sent_length_features\n",
    "length_ranges_DF['Sentence Length'] = [length_ranges_DF.iloc[row]['Feature'][0]['Sentence Length'] for row in range(len(length_ranges_DF.index))]\n",
    "length_ranges_DF['Category'] = [length_ranges_DF.iloc[row]['Feature'][1] for row in range(len(length_ranges_DF.index))]\n",
    "length_ranges_DF['Sentences'] = [1 for row in range(len(length_ranges_DF.index))]\n",
    "\n",
    "length_ranges_DF.loc[length_ranges_DF['Category'] == 'news', 'News'] = 1\n",
    "length_ranges_DF.loc[length_ranges_DF['Category'] == 'humor', 'Humor'] = 1\n",
    "\n",
    "#Sorter\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '0-5 Words', 'Sorter'] = 0\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '6-10 Words', 'Sorter'] = 1\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '10-15 Words', 'Sorter'] = 2\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '16-20 Words', 'Sorter'] = 3\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '21-30 Words', 'Sorter'] = 4\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '31-40 Words', 'Sorter'] = 5\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '41-50 Words', 'Sorter'] = 6\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '51-75 Words', 'Sorter'] = 7\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '76-100 Words', 'Sorter'] = 8\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == 'More Than 100 Words', 'Sorter'] = 9\n",
    "\n",
    "#Drop Feature and Category Columns\n",
    "length_ranges_DF = length_ranges_DF[['Sorter', 'Sentence Length', 'Sentences', 'News', 'Humor']]\n",
    "\n",
    "#Aggregate\n",
    "length_ranges_DF = length_ranges_DF.groupby(['Sorter', 'Sentence Length']).aggregate('count')\n",
    "\n",
    "#Drop Sorter Column\n",
    "length_ranges_DF.index = [index for (sorter,index) in length_ranges_DF.index]\n",
    "\n",
    "#Print results\n",
    "length_ranges_DF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of the total values, lets look at the proportion of categories among Sentence Length and also add which category has the highest proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>News</th>\n",
       "      <th>Humor</th>\n",
       "      <th>Most Frequent Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-5 Words</th>\n",
       "      <td>694</td>\n",
       "      <td>0.7867</td>\n",
       "      <td>0.2133</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-10 Words</th>\n",
       "      <td>854</td>\n",
       "      <td>0.7307</td>\n",
       "      <td>0.2693</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16-20 Words</th>\n",
       "      <td>1030</td>\n",
       "      <td>0.8388</td>\n",
       "      <td>0.1612</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21-30 Words</th>\n",
       "      <td>1330</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>0.1436</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-40 Words</th>\n",
       "      <td>565</td>\n",
       "      <td>0.8159</td>\n",
       "      <td>0.1841</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41-50 Words</th>\n",
       "      <td>148</td>\n",
       "      <td>0.7905</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51-75 Words</th>\n",
       "      <td>42</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76-100 Words</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>Even Split</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentences    News   Humor Most Frequent Category\n",
       "0-5 Words           694  0.7867  0.2133                   News\n",
       "6-10 Words          854  0.7307  0.2693                   News\n",
       "16-20 Words        1030  0.8388  0.1612                   News\n",
       "21-30 Words        1330  0.8564  0.1436                   News\n",
       "31-40 Words         565  0.8159  0.1841                   News\n",
       "41-50 Words         148  0.7905  0.2095                   News\n",
       "51-75 Words          42  0.6667  0.3333                   News\n",
       "76-100 Words          4  0.5000  0.5000             Even Split"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert to proportions\n",
    "length_ranges_DF['News'] = round(length_ranges_DF['News'] / length_ranges_DF['Sentences'], 4)\n",
    "length_ranges_DF['Humor'] = round(length_ranges_DF['Humor'] / length_ranges_DF['Sentences'], 4)\n",
    "\n",
    "#Add Most Common Category column\n",
    "length_ranges_DF.loc[length_ranges_DF['News'] == length_ranges_DF['Humor'], 'Most Frequent Category'] = 'Even Split'\n",
    "length_ranges_DF.loc[length_ranges_DF['News'] > length_ranges_DF['Humor'], 'Most Frequent Category'] = 'News'\n",
    "length_ranges_DF.loc[length_ranges_DF['News'] < length_ranges_DF['Humor'], 'Most Frequent Category'] = 'Humor'\n",
    "\n",
    "#Print results\n",
    "length_ranges_DF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the idea that there are more News sentences overall than Humor Sentences, which we saw in the DataFrame with the whole numbers of sentences. The last experiment will be to compare the proportion of news and humor sentences based on an even number of news and humor sentences. We will find whichever category has less, take all of those sentences, then take the same amount of the other category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>News</th>\n",
       "      <th>Humor</th>\n",
       "      <th>Most Frequent Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-5 Words</th>\n",
       "      <td>276</td>\n",
       "      <td>0.4638</td>\n",
       "      <td>0.5362</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-10 Words</th>\n",
       "      <td>379</td>\n",
       "      <td>0.3931</td>\n",
       "      <td>0.6069</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16-20 Words</th>\n",
       "      <td>377</td>\n",
       "      <td>0.5597</td>\n",
       "      <td>0.4403</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21-30 Words</th>\n",
       "      <td>440</td>\n",
       "      <td>0.5659</td>\n",
       "      <td>0.4341</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-40 Words</th>\n",
       "      <td>209</td>\n",
       "      <td>0.5024</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41-50 Words</th>\n",
       "      <td>55</td>\n",
       "      <td>0.4364</td>\n",
       "      <td>0.5636</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51-75 Words</th>\n",
       "      <td>22</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.6364</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76-100 Words</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentences    News   Humor Most Frequent Category\n",
       "0-5 Words           276  0.4638  0.5362                  Humor\n",
       "6-10 Words          379  0.3931  0.6069                  Humor\n",
       "16-20 Words         377  0.5597  0.4403                   News\n",
       "21-30 Words         440  0.5659  0.4341                   News\n",
       "31-40 Words         209  0.5024  0.4976                   News\n",
       "41-50 Words          55  0.4364  0.5636                  Humor\n",
       "51-75 Words          22  0.3636  0.6364                  Humor\n",
       "76-100 Words          2  0.0000  1.0000                  Humor"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of news sentences and humor sentences\n",
    "news_sents = [(featdict,category) for (featdict,category) in sent_length_features if category == 'news']\n",
    "humor_sents = [(featdict,category) for (featdict,category) in sent_length_features if category == 'humor']\n",
    "\n",
    "#Min between number of news sentences and number of humor sentences\n",
    "samenumber = min(len(news_sents),len(humor_sents))\n",
    "\n",
    "#Create new sentence features\n",
    "sent_length_features = news_sents[:samenumber] + humor_sents[:samenumber]\n",
    "\n",
    "#Create data frame\n",
    "length_ranges_DF = pandas.DataFrame(columns = ['Sorter', 'Feature', 'Sentence Length', 'Category', 'Sentences', 'News', 'Humor'])\n",
    "length_ranges_DF['Feature'] = sent_length_features\n",
    "length_ranges_DF['Sentence Length'] = [length_ranges_DF.iloc[row]['Feature'][0]['Sentence Length'] for row in range(len(length_ranges_DF.index))]\n",
    "length_ranges_DF['Category'] = [length_ranges_DF.iloc[row]['Feature'][1] for row in range(len(length_ranges_DF.index))]\n",
    "length_ranges_DF['Sentences'] = [1 for row in range(len(length_ranges_DF.index))]\n",
    "\n",
    "length_ranges_DF.loc[length_ranges_DF['Category'] == 'news', 'News'] = 1\n",
    "length_ranges_DF.loc[length_ranges_DF['Category'] == 'humor', 'Humor'] = 1\n",
    "\n",
    "#Sorter\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '0-5 Words', 'Sorter'] = 0\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '6-10 Words', 'Sorter'] = 1\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '10-15 Words', 'Sorter'] = 2\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '16-20 Words', 'Sorter'] = 3\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '21-30 Words', 'Sorter'] = 4\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '31-40 Words', 'Sorter'] = 5\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '41-50 Words', 'Sorter'] = 6\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '51-75 Words', 'Sorter'] = 7\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == '76-100 Words', 'Sorter'] = 8\n",
    "length_ranges_DF.loc[length_ranges_DF['Sentence Length'] == 'More Than 100 Words', 'Sorter'] = 9\n",
    "\n",
    "#Drop Feature and Category Columns\n",
    "length_ranges_DF = length_ranges_DF[['Sorter', 'Sentence Length', 'Sentences', 'News', 'Humor']]\n",
    "\n",
    "#Aggregate\n",
    "length_ranges_DF = length_ranges_DF.groupby(['Sorter', 'Sentence Length']).aggregate('count')\n",
    "\n",
    "#Drop Sorter Column\n",
    "length_ranges_DF.index = [index for (sorter,index) in length_ranges_DF.index]\n",
    "\n",
    "#Convert to proportions\n",
    "length_ranges_DF['News'] = round(length_ranges_DF['News'] / length_ranges_DF['Sentences'], 4)\n",
    "length_ranges_DF['Humor'] = round(length_ranges_DF['Humor'] / length_ranges_DF['Sentences'], 4)\n",
    "\n",
    "#Add Most Common Category column\n",
    "length_ranges_DF.loc[length_ranges_DF['News'] == length_ranges_DF['Humor'], 'Most Frequent Category'] = 'Even Split'\n",
    "length_ranges_DF.loc[length_ranges_DF['News'] > length_ranges_DF['Humor'], 'Most Frequent Category'] = 'News'\n",
    "length_ranges_DF.loc[length_ranges_DF['News'] < length_ranges_DF['Humor'], 'Most Frequent Category'] = 'Humor'\n",
    "\n",
    "#Print results\n",
    "length_ranges_DF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is some good stuff! We can see here that generally humor has either shorter or longer sentences, and news takes more of the middle. This makes sense since jokes are usually either \"one-liners\" or take a while to \"set up\", and news is much more predictable in terms of how long it will be, plus is usually much more formal and standardized than humor is.\n",
    "<br>\n",
    "Finally, the very very last step to take would be to create classifiers and test them against these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Sentence Length Classifier\n",
      "Accuracy - 37.5 %\n"
     ]
    }
   ],
   "source": [
    "#Create test feature sets\n",
    "test_features = [({'Sentence Length' : index}, length_ranges_DF.loc[index]['Most Frequent Category'].lower()) for index in length_ranges_DF.index]\n",
    "\n",
    "#Create train features\n",
    "train_features = [feature for feature in featuresDF['Actual Feature'] if list(feature[0].keys())[0] == 'Sentence Length' and feature[1] in ('news','humor')]\n",
    "train_features\n",
    "\n",
    "#Create classifier\n",
    "print('Updated Sentence Length Classifier')\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "accuracy = nltk.classify.accuracy(classifier, test_features)\n",
    "print('Accuracy -', round(accuracy*100, 2), '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great!!! Not the best accuracy, but pretty impressive given so few rules to test against!\n",
    "<br>\n",
    "<br>\n",
    "CONCLUSION:\n",
    "<br>\n",
    "While countless more testing and experiments could be done, we still learned a lot during this project! It is important to identify what features or attributes will be of interest as we unforunatley spent a decent portion of this project getting to the point where we settled on sentnece length ranges. This is likely just a byproduct of the fact that our classifiers used supervised machine learning. Ideally, all attributes would be stored in a data frame and unsupervised learning would be done to discover which are of best interest to be more efficient in this process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JUPYTER",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
